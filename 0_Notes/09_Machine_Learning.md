# Machine Learning

## Tag 1

- 10: 
  - beim tictactoe ist es möglich das perfekt zu machen, 
  - Wenn es perfekt möglich ist dann sind wir meistens nicht in ML
- 11:
  - Schach ist zu komplex um alles durch-zusimulieren
  - Perfekte lösung nicht möglich, aber eine sehr gute lösung ist gut genug
  - Idee von perfektion wird aufgegeben
- 12:
  - räpresentieren das objekt in der reelen welt mit features
  - die features sind wichtig, aber es gibt immer ein informationsverlust aber meistens können wir es gut genug räpresentieren
- 13:
  - modell kann ausprogrammiert sein (imperativer code mit if, else, etc.). Ist immernoch AI weil es intelligent erscheint. Obwohl es nicht gelernt ist.
  - Wie ein modell lernt ist auch ausprogrammiert sein
- 14:
  - AI -> Tic Tac Toe
  - ML -> Schach-Spiel
  - Deep Learning -> Das Modell ist zusätzlich ein neurales netz
- 15:
  - Im kurs nehmen wir immer an, dass die Daten sauber sind und korrekt sind (Clean Data)
- 17:
  - 1 -> Hunderte Daten aber viel Wissen
  - 3 -> eigentlich alles aus den Daten, in den bereichen bei denen die Daten schwierig ist zu definieren. Eg. Bilderkennung, haben wir pixel es ist zu klein um zu wissen was ein hund ist, daher wird mehr gelernt
  - Im praxis alltag (asser grosse player) sind wir bei 1 & 2
  - Wir haben aber immer ein paar annahmen und daten
- 19:
  - Vorteil mehr kontrolle, und wir können analysieren wo es gut ist. Und dann gezielt verbessern
  - Nachteil modell kann mit der zeit out of date sein
  - Meistens offline in der realen welt weil es einfacher ist
  - Beispiel online learning dass schief ging, Microsoft AI dass ein Twitter bot gab, der rassistisch wurde
- 20: 
  - wir sind im bereich von 5-6
  - Aber 3 & 4 müssen auch miteinbezogen werden
  - Visualisierung ist auch gut um bugs zu finden, die auch in den daten sein können
- 22
  - Möchten output vorhersagen
  - Abhängige Variabel -> output
- 24:
  - kontinuierlich -> eg. Preis eine Zahl
- 25:
  - expertenwissen zum beispiel dass es kein negatives gewicht
- 26:
  - Encoden -> zu beispiel text -> die fischart
  - Standardisiert -> nummern
- 30
  - Betas ist das was wir lernen
  - das modell kann die formel nicht verändern sondern nur die betas lernen
- 34:
  - Gewichtete Summe der Features -> unterschiedliche "wichtigkeit"
  - es gibt keine interaktion zwischen den features, das ist auch die Schwäche des Modells
- 35
  - echtes gewicht = vorhersage + fehler = modell formel + fehler
  - jetzt kann man über den fehler eine vorhersage machen
- 36
  - Alles dieselbe Former in anderen notationen, das Lineare Model
  - 3. kann es auch als vekoren schreiben
  - 4. Wir wissen dass es vektoren sind, wir lassen es weg
- 37
  - Visualisierung ist gut für die Intuition aber es hört schnell auf
- 39
  - 1: summe von gewichtungen * features
  - 2: Nein kann auch eine fläche sein (hyper-ebene)

### Regressions Metrik

- 41
  - fehler pro datenpunkt kann einfach gemessen werden
  - residual (epsilon) -> fehler
  - Der fehler pro datenpunkt ist nicht genügend um zu beurteilen ob es gut ist
  - Es muss zusammengezählt werden -> Metrik
- 42
  - Verschiedene Fragen um die Metrik zu erstellen
  - Wir brauchen eine Metrik um zu entscheiden was besser ist.
  - Die metrik ist stark abhängig von der Fragestellung
  - Häuserpreise, sollen wir das Schloss wegnehmen?
  - Müssen aussreisser mit einbezogen werden? Dann andere metrik
- 43
  - Mean Absolute Error (alle fehler zusammengezählt) MAE 
    - -> Robust gegen ausreisser
  - Mean Squared Error (Summe aller quadrate) MSE 
    - -> nimmt ausreisser miteinbezogen und werden davon beeinflusst
  - Das sind standard metriken, es können auch custom sein, zb. ab 20'000 franken muss es genauer sein, etc.
- 45
  - Was sind die Kosten von unseren Parameter? -> gleich einer metrik
  - beta sind parameter und nicht die vorhersage
  - kosten meinen eigentlich dass es schlecht ist je höher
  - Schlechte betas hohe zahl (hohe kosten) das ist schlecht
  - was für ein fehler machen wir auf dem datensatz?
  - Kostenfunktion ist vergleich zu der gesetzten metrik parameterisiert mit den betas
- 47
  - Bei der Linearen Funktion ist die Kostenfunktion normalerweise der MSE die Kostenfunktion
  - Das wird normalerweise genommen, oftmals auch normalverteilt
  - sklearn hat das hard-coded
- 49
  - Metrk ist eine Zahl
- 50
  - Optimum wie wir die lernbaren parameter finden basierend auf einer Kostenfunktion und daten
  - Unterschiedliche algorythmen die unterschiedliche vorteile haben, meistens wird ein schneller verwendet
- 52
  - Jeder punkt auf der fläche ist eine andere gerade und jeder punkt sagt wie die kosten für diese kobination
  - Der tiefste punkt sind die besten betas -> kleinste fehler
  - Nicht bei allen modellen ist die kostenfunktion eine schüssel
- 53
  - Wo ist die Ableitung (steigung) null ist das beste beta
- 55: Gradient Descent
  - Andere optimierungs algorithums
  - iterativ, wir starten an einem punkt (zufällige betas)
  - wir haben am anfang eine zufällige linie
  - ist vielleicht noch nicht gut
  - Wir machen dann einen schritt in eine richtung -> neue lösung die leicht besser ist aber immernoch nicht gut
  - mit jedem schritt wird es etwas besser
  - macht das so lange bis wir zum minimum kommen
  - wir schauen wo es am steilsten runter geht, das funktioniert mit der ableitung
- 56
  - Analysis gibt die steigung der kosten funktion, mit mehreren variabeln muss mehrmals abgeleitet werden
  - Bei einer kurve ist die ableitung die tangente zu dem punkt
- 57
  - wir machen die funktion minus weil wir runter laufen wollen das "n" ist die schrittgrösse
  - es gibt verschiedene algorythem um eine lösung zu finden, wir nehmen normaler weise die die schneller ist weil wir sind am resultat interessiert und nicht am weg
- 58
  - Ist immer eine linie, weil es linear ist, andere modelle können auch ein krümmung lernen
  - Zwei features: ebene, kann auch keine krümmung lernen und sich mehr an die daten anpassen
  - die gefahr für overfitting ist kleiner bei linearen modellen als bei anderen weil sie starrer sind
- 61
  - 1:
    - lineare modell nimmt die lineare funktion an
    - wie wir das modell machen ist hard-coded weil es zwingend eine lineare formel nimmt
    - wir nehmen an das es eine lösung hat (die ableitung = 0)
    - nimmt meistens der MSE an (kann man aber auch leicht ändern) aber das ist meistens implizit mit definiert
  - 2:
    - betas
    - formel ist ausprogrammiert, die betas werden aus den daten gelernt
  - 3:
    - Gradient Descent, Analytisch mit dem minimum
- 64
  - genauigkeit der daten kann erhalten werden
- 65
  - Kategorische Features
- 66
  - Ordinal encoding
  - man hat einen text und will ihn in eine zahl verwandeln weil wir mit text nicht rechnen können
  - Zahl 0-n
  - 1 feature wird 1 neues feature, es sind kategorien aber NICHT eine ordnung (reihenfolge) oder unterschiedliche grössen
  - 3 kategorien 0, 1, 2
  - 1:1 feature aber ist nicht gut wenn es keine ordnung gibt, wenn es eine ordnung gibt kann es gut sein, so lange der die gaps zwischen den kategorien konsistent ist
- 67
  - One-hot-encoding
  - Auf einen n dimensionalen vektor
  - 3 Kategorien
  - 3 Vektorne [1, 0, 0] und [0, 1, 0] und [0, 0, 1]
  - Drei neue features 0 oder 1 an einer spezifischen stelle im vektor
  - 1 neues feature pro kategorie: ist besser wenn es keine ordnung gibt weil dann die hierarchy einen einfluss haben kann obwohl sie das nicht sollte.
- 68
  - Man will meistens keine Ordnung haben
  - Es ist auch möglich ein eigenes encoding zu machen mit domain knowledge
  - zb. 100 verschiedene Haus fassaden, werden auf 5 verschiedene eigenschaften gemapped, zb. Wert oder so
  - generiert hier 5 neue features, ist eine art expertenwissen einzubeziehen, aber ist natürlich sehr sensibel darauf das es korrekt ist
  - wenn wir die features von den kategorien mit ordinal machen, dann hat das dritte mit nummer 2 einen grösseren einfluss als 
  - `y = b0 + b1x1 + b2x2 ...` wenn x1 ordinal 1 ist und dann x2 ordinal 2 hat es einen grösseren einfluss, mit vektoren würde es nur einen einfluss auf das beta haben wenn es zu der kategorie gehört. Also fällen die parameter weg die sich gar nicht dazu bezeien

### Explizites Feature Engineering

- 73
  - Menge an gesammelten features, aus diesen gibt es ein neues feature, welches aber nicht in der reellen welt gesammelt wird.
  - Standard: z.b. aus der grösse des fisches können wir das volumen berechnen, das wäre dann hilfreich für das modell wenn es das gewicht schätzen will
  - In das feature engineering fliesst das expertenwissen rein, sodass man das richtige feature engineered wird
  - Man sagt dem modell teile der Kausalität sodass es es nicht alleine erraten muss. Also ein teil des learning wird abgenommen.
  - Es gibt so bessere Modelle
- Polynomielle Regression: x1, x2, (x1)^2, (x2)^2, x1*x2 (zu einem -n degree)
  - Man kann auch nur einen teil der berechnungen machen wenn sie so sinn machen, z.b. das volumen des fisches
  - Zuerst mit polynomien feature engineering, dann lienare regression, das ist ein neues modell 
- Code (poly_regression): 
  - skikit spezifisch: `sk.Pipeline` sagt, welche modelle es in welcher reihenfolge es macht
  - Jetzt hat es eine krümmung gelernt -> (x1)^2
  - Feature Space: Die features die in das modell gehen, es können variabeln drinn haben die engineert sind

### Modell evaluierung

- 79:
  - Auf den trainings daten ist es immer besser oder gleich gut
  - eher overfitting
  - Auf ungesehenen daten kann es schlechter sein, das ist ein grösseres problem
  - es kann die gewichtung eines feature vergrössern
  - das wichtigste ist jedoch ob es gut auf neuen daten ist
- 81
  - Random Forest ist am besten auf den trainingsdaten am besten, aber es ist krass overfittet auf neuen daten wird es komplett unbrauchbar sein
- 82
  - gleiche trainingsdaten, aber neue validierungsdaten andere fitts und es ist klar das random forest am schlechtesten ist
  - wird eigentlich immer auf neuen daten gerechnet, aber wenn die evaluierungsdaten sehr wenige sind oder viele ausreisser haben, kann das ergebnis verfälschen
- 86
  - Cross validation: trainings daten und dann validierungsdaten
- 87
  - Es gibt eine gefahr dass das modell durch das validierungsset nur durch zufall besser abschneidet
- 88
  - Zwei strategien -> noch ein weiteres test set
  - Oder grössere validation sets, dann aber weniger trainingsdaten...
- 89
  - train (training), 
  - validation (welches modell ist am besten), 
  - test (schauen ob es wirklich gut ist, bei daten die 0 einfluss haben), werden nie angeschaut
  - -> in der praxis am besten
  - wenn ein modell verworfen werden wegen einem test set, hat es einen einfluss auf das moell und muss weggeworfen werden und ein neues test set gesammelt werden
  - man kann auch auf ein validierungsset overfitten
- 90
  - "Herbei zaubern von test daten"
  - Wir möchten ein möglichst grosses validierungsset haben damit wir ein model nicht nur zufällig wählen
  - Einen trick das validierungsset zu vergrössern
  - K-Fold Cross Validation eine methodik
  - Alle daten sind validierung
- 91
  - 1, 2, 3, 4, 5 gelbes sind die validierungssets
  - Wir haben verschiedene arten von modellen
  - Ein model, zb. Lineare Regression trainieren sie 5 mal auf verschiedene teile vom Datenset
  - jedes model hat die validierungsdaten während dem training nicht gesehen
  - Dann werden die vorhersagen (von den verschiedenen modell arten verglichen, linear vs. polynom, etc) verglichen, wir wählen eine Modell art
  - Dann können wir die resultate der Modelle zusammen nehmen, oder eines auswählen oder wir können ein neues modell trainieren auf allen daten (keine validierungsdaten) weil wir es ja vorher validiert haben
- Hold-Out Cross validation -> train & validate
- K-fold corss validation -> gemäss 91
- 93
  - Komplex -> wie stark passt sich das model an? Wie flexibel ist es?
  - Sehr komplexes model & test schlecht -> overfitting
  - Sehr einfaches model beides nicht besonders gut -> underfitting
  - Also die komplexität richtig wählen
  - Model based view: Das model ist im zentrum, daten sind fix
  - Aber wir können nicht nur die modelle anpassen sondern auch neue daten erheben oder feature engineering
- 96
  - 1. Modell ist nicht gut es ist zu einfach, modelle sind tendenziell zu einfach
  - 2. Modell ist sehr gut auf test daten aber in der realität (neue daten) nicht. Modelle die overfittet sind, sind tendenziell komplexer
  - 3. Validierungsdaten sind schlecht
  - 4. Daten auf die seite
  - 5. Iterative über alle segmente, alles ist einmal im validierungsset, alles ist vier mal im trainings set


### Feature selection

- 98
  - nur ein teil der features werden genommen
  - macht es dem modell einfacher und das modell wird selbst einfacher
  - overfittet nicht auf zufällige features die nichts damit zu tun haben
- 101
  - grosse betas sind tendenziell eine indikation dass es overfittet ist

- 102 & 103
  - Die features müssen irgendwie standardisiert werden
  - Alle betas haben das gleiche lambda
- 102
  - L-1 Regularisierung
  - La Place verteilung
  - Wenn es ein sehr grosses beta ist, wird es durch diese kosten funktion bestraft
  - je grösser das beta desto höher die kosten funktion zusätzliche strafe in unsere kostenfunktion
  - lambda ist ein hyperparameter und wird nicht gelernt sondern von uns gesetzt. ist eine konstante
  - das modell sieht anders bei anderen lambdas
  - L-1 ist gut für feature selection, wenn ein beta nicht wichtig ist (feature) dann wird es auf null gesetzt schlussendlich
- 103: L-2 Regularisierung
  - Normalverteilung
  - betas sollten nahe 0 sind sonst wird es bestraft
  - Standardisierung ist quadratisch, wir machen wie vorher eine straffunktion dazu
  - Es wird dazu gezwungen kleinere betas zu nehmen
  - Zahlen unter 1 sind weniger streng bestraft bei L2 als bei L1
- 105
  - Standardisierung
  - Wie gross die betas werden hänge davon ab wie gross die gemessene daten sind
  - Wenn breite in meter und länge in cm dann hat die einheit einen einfluss auf die gewichtung obwohl es das nicht sollte
  - Also eigentlich die einheiten weg-werfen
- 106
  - Wenn standardisiert wird ist best practice alle einheiten losgeworden
  - z.b. wir rechnen es um so dass die werte vergleichbar sind, zum beispiel können wir grösse und geld nicht vergleichen
  - Wenn sie standardisiert werden, dann ist nur noch ein verhältnis übrig, dass nichts mehr mit den einheiten zu tun hat und sie können verglichen werden
  - die werte sind jetzt intutiv nicht mehr nachvollziehbar, die betas können auch nicht wirklich verstanden werden
  - Wenn die betas verstanden werden wollen, dann müssten wir noch viel machen, es geht nicht so einfach
  - Standard Scaler kann man als einen teil der pipeline vorstellen
  - Wenn standardisiert wird, muss dann der input des modells in der anwendung auch standardisiert werden
- 

