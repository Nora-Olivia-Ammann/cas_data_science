# Machine Learning

## Tag 1

- 10: 
  - beim tictactoe ist es möglich das perfekt zu machen, 
  - Wenn es perfekt möglich ist dann sind wir meistens nicht in ML
- 11:
  - Schach ist zu komplex um alles durch-zusimulieren
  - Perfekte lösung nicht möglich, aber eine sehr gute lösung ist gut genug
  - Idee von perfektion wird aufgegeben
- 12:
  - räpresentieren das objekt in der reelen welt mit features
  - die features sind wichtig, aber es gibt immer ein informationsverlust aber meistens können wir es gut genug räpresentieren
- 13:
  - modell kann ausprogrammiert sein (imperativer code mit if, else, etc.). Ist immernoch AI weil es intelligent erscheint. Obwohl es nicht gelernt ist.
  - Wie ein modell lernt ist auch ausprogrammiert sein
- 14:
  - AI -> Tic Tac Toe
  - ML -> Schach-Spiel
  - Deep Learning -> Das Modell ist zusätzlich ein neurales netz
- 15:
  - Im kurs nehmen wir immer an, dass die Daten sauber sind und korrekt sind (Clean Data)
- 17:
  - 1 -> Hunderte Daten aber viel Wissen
  - 3 -> eigentlich alles aus den Daten, in den bereichen bei denen die Daten schwierig ist zu definieren. Eg. Bilderkennung, haben wir pixel es ist zu klein um zu wissen was ein hund ist, daher wird mehr gelernt
  - Im praxis alltag (asser grosse player) sind wir bei 1 & 2
  - Wir haben aber immer ein paar annahmen und daten
- 19:
  - Vorteil mehr kontrolle, und wir können analysieren wo es gut ist. Und dann gezielt verbessern
  - Nachteil modell kann mit der zeit out of date sein
  - Meistens offline in der realen welt weil es einfacher ist
  - Beispiel online learning dass schief ging, Microsoft AI dass ein Twitter bot gab, der rassistisch wurde
- 20: 
  - wir sind im bereich von 5-6
  - Aber 3 & 4 müssen auch miteinbezogen werden
  - Visualisierung ist auch gut um bugs zu finden, die auch in den daten sein können
- 22
  - Möchten output vorhersagen
  - Abhängige Variabel -> output
- 24:
  - kontinuierlich -> eg. Preis eine Zahl
- 25:
  - expertenwissen zum beispiel dass es kein negatives gewicht
- 26:
  - Encoden -> zu beispiel text -> die fischart
  - Standardisiert -> nummern
- 30
  - Betas ist das was wir lernen
  - das modell kann die formel nicht verändern sondern nur die betas lernen
- 34:
  - Gewichtete Summe der Features -> unterschiedliche "wichtigkeit"
  - es gibt keine interaktion zwischen den features, das ist auch die Schwäche des Modells
- 35
  - echtes gewicht = vorhersage + fehler = modell formel + fehler
  - jetzt kann man über den fehler eine vorhersage machen
- 36
  - Alles dieselbe Former in anderen notationen, das Lineare Model
  - 3. kann es auch als vekoren schreiben
  - 4. Wir wissen dass es vektoren sind, wir lassen es weg
- 37
  - Visualisierung ist gut für die Intuition aber es hört schnell auf
- 39
  - 1: summe von gewichtungen * features
  - 2: Nein kann auch eine fläche sein (hyper-ebene)

### Regressions Metrik

- 41
  - fehler pro datenpunkt kann einfach gemessen werden
  - residual (epsilon) -> fehler
  - Der fehler pro datenpunkt ist nicht genügend um zu beurteilen ob es gut ist
  - Es muss zusammengezählt werden -> Metrik
- 42
  - Verschiedene Fragen um die Metrik zu erstellen
  - Wir brauchen eine Metrik um zu entscheiden was besser ist.
  - Die metrik ist stark abhängig von der Fragestellung
  - Häuserpreise, sollen wir das Schloss wegnehmen?
  - Müssen aussreisser mit einbezogen werden? Dann andere metrik
- 43
  - Mean Absolute Error (alle fehler zusammengezählt) MAE 
    - -> Robust gegen ausreisser
  - Mean Squared Error (Summe aller quadrate) MSE 
    - -> nimmt ausreisser miteinbezogen und werden davon beeinflusst
  - Das sind standard metriken, es können auch custom sein, zb. ab 20'000 franken muss es genauer sein, etc.
- 45
  - Was sind die Kosten von unseren Parameter? -> gleich einer metrik
  - beta sind parameter und nicht die vorhersage
  - kosten meinen eigentlich dass es schlecht ist je höher
  - Schlechte betas hohe zahl (hohe kosten) das ist schlecht
  - was für ein fehler machen wir auf dem datensatz?
  - Kostenfunktion ist vergleich zu der gesetzten metrik parameterisiert mit den betas
- 47
  - Bei der Linearen Funktion ist die Kostenfunktion normalerweise der MSE die Kostenfunktion
  - Das wird normalerweise genommen, oftmals auch normalverteilt
  - sklearn hat das hard-coded
- 49
  - Metrk ist eine Zahl
- 50
  - Optimum wie wir die lernbaren parameter finden basierend auf einer Kostenfunktion und daten
  - Unterschiedliche algorythmen die unterschiedliche vorteile haben, meistens wird ein schneller verwendet
- 52
  - Jeder punkt auf der fläche ist eine andere gerade und jeder punkt sagt wie die kosten für diese kobination
  - Der tiefste punkt sind die besten betas -> kleinste fehler
  - Nicht bei allen modellen ist die kostenfunktion eine schüssel
- 53
  - Wo ist die Ableitung (steigung) null ist das beste beta
- 55: Gradient Descent
  - Andere optimierungs algorithums
  - iterativ, wir starten an einem punkt (zufällige betas)
  - wir haben am anfang eine zufällige linie
  - ist vielleicht noch nicht gut
  - Wir machen dann einen schritt in eine richtung -> neue lösung die leicht besser ist aber immernoch nicht gut
  - mit jedem schritt wird es etwas besser
  - macht das so lange bis wir zum minimum kommen
  - wir schauen wo es am steilsten runter geht, das funktioniert mit der ableitung
- 56
  - Analysis gibt die steigung der kosten funktion, mit mehreren variabeln muss mehrmals abgeleitet werden
  - Bei einer kurve ist die ableitung die tangente zu dem punkt
- 57
  - wir machen die funktion minus weil wir runter laufen wollen das "n" ist die schrittgrösse
  - es gibt verschiedene algorythem um eine lösung zu finden, wir nehmen normaler weise die die schneller ist weil wir sind am resultat interessiert und nicht am weg
- 58
  - Ist immer eine linie, weil es linear ist, andere modelle können auch ein krümmung lernen
  - Zwei features: ebene, kann auch keine krümmung lernen und sich mehr an die daten anpassen
  - die gefahr für overfitting ist kleiner bei linearen modellen als bei anderen weil sie starrer sind
- 61
  - 1:
    - lineare modell nimmt die lineare funktion an
    - wie wir das modell machen ist hard-coded weil es zwingend eine lineare formel nimmt
    - wir nehmen an das es eine lösung hat (die ableitung = 0)
    - nimmt meistens der MSE an (kann man aber auch leicht ändern) aber das ist meistens implizit mit definiert
  - 2:
    - betas
    - formel ist ausprogrammiert, die betas werden aus den daten gelernt
  - 3:
    - Gradient Descent, Analytisch mit dem minimum
- 64
  - genauigkeit der daten kann erhalten werden
- 65
  - Kategorische Features
- 66
  - Ordinal encoding
  - man hat einen text und will ihn in eine zahl verwandeln weil wir mit text nicht rechnen können
  - Zahl 0-n
  - 1 feature wird 1 neues feature, es sind kategorien aber NICHT eine ordnung (reihenfolge) oder unterschiedliche grössen
  - 3 kategorien 0, 1, 2
  - 1:1 feature aber ist nicht gut wenn es keine ordnung gibt, wenn es eine ordnung gibt kann es gut sein, so lange der die gaps zwischen den kategorien konsistent ist
- 67
  - One-hot-encoding
  - Auf einen n dimensionalen vektor
  - 3 Kategorien
  - 3 Vektorne [1, 0, 0] und [0, 1, 0] und [0, 0, 1]
  - Drei neue features 0 oder 1 an einer spezifischen stelle im vektor
  - 1 neues feature pro kategorie: ist besser wenn es keine ordnung gibt weil dann die hierarchy einen einfluss haben kann obwohl sie das nicht sollte.
- 68
  - Man will meistens keine Ordnung haben
  - Es ist auch möglich ein eigenes encoding zu machen mit domain knowledge
  - zb. 100 verschiedene Haus fassaden, werden auf 5 verschiedene eigenschaften gemapped, zb. Wert oder so
  - generiert hier 5 neue features, ist eine art expertenwissen einzubeziehen, aber ist natürlich sehr sensibel darauf das es korrekt ist
  - wenn wir die features von den kategorien mit ordinal machen, dann hat das dritte mit nummer 2 einen grösseren einfluss als 
  - `y = b0 + b1x1 + b2x2 ...` wenn x1 ordinal 1 ist und dann x2 ordinal 2 hat es einen grösseren einfluss, mit vektoren würde es nur einen einfluss auf das beta haben wenn es zu der kategorie gehört. Also fällen die parameter weg die sich gar nicht dazu bezeien

### Explizites Feature Engineering

- 73
  - Menge an gesammelten features, aus diesen gibt es ein neues feature, welches aber nicht in der reellen welt gesammelt wird.
  - Standard: z.b. aus der grösse des fisches können wir das volumen berechnen, das wäre dann hilfreich für das modell wenn es das gewicht schätzen will
  - In das feature engineering fliesst das expertenwissen rein, sodass man das richtige feature engineered wird
  - Man sagt dem modell teile der Kausalität sodass es es nicht alleine erraten muss. Also ein teil des learning wird abgenommen.
  - Es gibt so bessere Modelle
- Polynomielle Regression: x1, x2, (x1)^2, (x2)^2, x1*x2 (zu einem -n degree)
  - Man kann auch nur einen teil der berechnungen machen wenn sie so sinn machen, z.b. das volumen des fisches
  - Zuerst mit polynomien feature engineering, dann lienare regression, das ist ein neues modell 
- Code (poly_regression): 
  - skikit spezifisch: `sk.Pipeline` sagt, welche modelle es in welcher reihenfolge es macht
  - Jetzt hat es eine krümmung gelernt -> (x1)^2
- 
- 


