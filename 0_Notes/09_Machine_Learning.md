# Machine Learning

## Tag 1

### Übersicht Tag 1

- Supervised Learning
  - `X` Daten, `y` labels die wir schlussendlich predicten wollen
  - Preprocessing
    - Standardisierung
    - Encoding
    - Feature Selection
    - Feature Engineering (polynomisierung, etc)
  - Modell
    - eg. Linear Regression -> gibt eine Kontinuierliche Variabel
    - y-hat -> vorhersage
    - betas
  - Resultat
    - `y` neues
  - Metrik
    - hat eine kosten funktion die sagt wie gut die betas sind


- 10: 
  - beim tictactoe ist es möglich das perfekt zu machen, 
  - Wenn es perfekt möglich ist dann sind wir meistens nicht in ML
- 11:
  - Schach ist zu komplex um alles durch-zusimulieren
  - Perfekte lösung nicht möglich, aber eine sehr gute lösung ist gut genug
  - Idee von perfektion wird aufgegeben
- 12:
  - räpresentieren das objekt in der reelen welt mit features
  - die features sind wichtig, aber es gibt immer ein informationsverlust aber meistens können wir es gut genug räpresentieren
- 13:
  - modell kann ausprogrammiert sein (imperativer code mit if, else, etc.). Ist immernoch AI weil es intelligent erscheint. Obwohl es nicht gelernt ist.
  - Wie ein modell lernt ist auch ausprogrammiert sein
- 14:
  - AI -> Tic Tac Toe
  - ML -> Schach-Spiel
  - Deep Learning -> Das Modell ist zusätzlich ein neurales netz
- 15:
  - Im kurs nehmen wir immer an, dass die Daten sauber sind und korrekt sind (Clean Data)
- 17:
  - 1 -> Hunderte Daten aber viel Wissen
  - 3 -> eigentlich alles aus den Daten, in den bereichen bei denen die Daten schwierig ist zu definieren. Eg. Bilderkennung, haben wir pixel es ist zu klein um zu wissen was ein hund ist, daher wird mehr gelernt
  - Im praxis alltag (asser grosse player) sind wir bei 1 & 2
  - Wir haben aber immer ein paar annahmen und daten
- 19:
  - Vorteil mehr kontrolle, und wir können analysieren wo es gut ist. Und dann gezielt verbessern
  - Nachteil modell kann mit der zeit out of date sein
  - Meistens offline in der realen welt weil es einfacher ist
  - Beispiel online learning dass schief ging, Microsoft AI dass ein Twitter bot gab, der rassistisch wurde
- 20: 
  - wir sind im bereich von 5-6
  - Aber 3 & 4 müssen auch miteinbezogen werden
  - Visualisierung ist auch gut um bugs zu finden, die auch in den daten sein können
- 22
  - Möchten output vorhersagen
  - Abhängige Variabel -> output
- 24:
  - kontinuierlich -> eg. Preis eine Zahl
- 25:
  - expertenwissen zum beispiel dass es kein negatives gewicht
- 26:
  - Encoden -> zu beispiel text -> die fischart
  - Standardisiert -> nummern
- 30
  - Betas ist das was wir lernen
  - das modell kann die formel nicht verändern sondern nur die betas lernen
- 34:
  - Gewichtete Summe der Features -> unterschiedliche "wichtigkeit"
  - es gibt keine interaktion zwischen den features, das ist auch die Schwäche des Modells
- 35
  - echtes gewicht = vorhersage + fehler = modell formel + fehler
  - jetzt kann man über den fehler eine vorhersage machen
- 36
  - Alles dieselbe Former in anderen notationen, das Lineare Model
  - 3. kann es auch als vekoren schreiben
  - 4. Wir wissen dass es vektoren sind, wir lassen es weg
- 37
  - Visualisierung ist gut für die Intuition aber es hört schnell auf
- 39
  - 1: summe von gewichtungen * features
  - 2: Nein kann auch eine fläche sein (hyper-ebene)

### Regressions Metrik

- 41
  - fehler pro datenpunkt kann einfach gemessen werden
  - residual (epsilon) -> fehler
  - Der fehler pro datenpunkt ist nicht genügend um zu beurteilen ob es gut ist
  - Es muss zusammengezählt werden -> Metrik
- 42
  - Verschiedene Fragen um die Metrik zu erstellen
  - Wir brauchen eine Metrik um zu entscheiden was besser ist.
  - Die metrik ist stark abhängig von der Fragestellung
  - Häuserpreise, sollen wir das Schloss wegnehmen?
  - Müssen aussreisser mit einbezogen werden? Dann andere metrik
- 43
  - Mean Absolute Error (alle fehler zusammengezählt) MAE 
    - -> Robust gegen ausreisser
  - Mean Squared Error (Summe aller quadrate) MSE 
    - -> nimmt ausreisser miteinbezogen und werden davon beeinflusst
  - Das sind standard metriken, es können auch custom sein, zb. ab 20'000 franken muss es genauer sein, etc.
- 45
  - Was sind die Kosten von unseren Parameter? -> gleich einer metrik
  - beta sind parameter und nicht die vorhersage
  - kosten meinen eigentlich dass es schlecht ist je höher
  - Schlechte betas hohe zahl (hohe kosten) das ist schlecht
  - was für ein fehler machen wir auf dem datensatz?
  - Kostenfunktion ist vergleich zu der gesetzten metrik parameterisiert mit den betas
- 47
  - Bei der Linearen Funktion ist die Kostenfunktion normalerweise der MSE die Kostenfunktion
  - Das wird normalerweise genommen, oftmals auch normalverteilt
  - sklearn hat das hard-coded
- 49
  - Metrk ist eine Zahl
- 50
  - Optimum wie wir die lernbaren parameter finden basierend auf einer Kostenfunktion und daten
  - Unterschiedliche algorythmen die unterschiedliche vorteile haben, meistens wird ein schneller verwendet
- 52
  - Jeder punkt auf der fläche ist eine andere gerade und jeder punkt sagt wie die kosten für diese kobination
  - Der tiefste punkt sind die besten betas -> kleinste fehler
  - Nicht bei allen modellen ist die kostenfunktion eine schüssel
- 53
  - Wo ist die Ableitung (steigung) null ist das beste beta
- 55: Gradient Descent
  - Andere optimierungs algorithums
  - iterativ, wir starten an einem punkt (zufällige betas)
  - wir haben am anfang eine zufällige linie
  - ist vielleicht noch nicht gut
  - Wir machen dann einen schritt in eine richtung -> neue lösung die leicht besser ist aber immernoch nicht gut
  - mit jedem schritt wird es etwas besser
  - macht das so lange bis wir zum minimum kommen
  - wir schauen wo es am steilsten runter geht, das funktioniert mit der ableitung
- 56
  - Analysis gibt die steigung der kosten funktion, mit mehreren variabeln muss mehrmals abgeleitet werden
  - Bei einer kurve ist die ableitung die tangente zu dem punkt
- 57
  - wir machen die funktion minus weil wir runter laufen wollen das "n" ist die schrittgrösse
  - es gibt verschiedene algorythem um eine lösung zu finden, wir nehmen normaler weise die die schneller ist weil wir sind am resultat interessiert und nicht am weg
- 58
  - Ist immer eine linie, weil es linear ist, andere modelle können auch ein krümmung lernen
  - Zwei features: ebene, kann auch keine krümmung lernen und sich mehr an die daten anpassen
  - die gefahr für overfitting ist kleiner bei linearen modellen als bei anderen weil sie starrer sind
- 61
  - 1:
    - lineare modell nimmt die lineare funktion an
    - wie wir das modell machen ist hard-coded weil es zwingend eine lineare formel nimmt
    - wir nehmen an das es eine lösung hat (die ableitung = 0)
    - nimmt meistens der MSE an (kann man aber auch leicht ändern) aber das ist meistens implizit mit definiert
  - 2:
    - betas
    - formel ist ausprogrammiert, die betas werden aus den daten gelernt
  - 3:
    - Gradient Descent, Analytisch mit dem minimum
- 64
  - genauigkeit der daten kann erhalten werden
- 65
  - Kategorische Features
- 66
  - Ordinal encoding
  - man hat einen text und will ihn in eine zahl verwandeln weil wir mit text nicht rechnen können
  - Zahl 0-n
  - 1 feature wird 1 neues feature, es sind kategorien aber NICHT eine ordnung (reihenfolge) oder unterschiedliche grössen
  - 3 kategorien 0, 1, 2
  - 1:1 feature aber ist nicht gut wenn es keine ordnung gibt, wenn es eine ordnung gibt kann es gut sein, so lange der die gaps zwischen den kategorien konsistent ist
- 67
  - One-hot-encoding
  - Auf einen n dimensionalen vektor
  - 3 Kategorien
  - 3 Vektorne [1, 0, 0] und [0, 1, 0] und [0, 0, 1]
  - Drei neue features 0 oder 1 an einer spezifischen stelle im vektor
  - 1 neues feature pro kategorie: ist besser wenn es keine ordnung gibt weil dann die hierarchy einen einfluss haben kann obwohl sie das nicht sollte.
- 68
  - Man will meistens keine Ordnung haben
  - Es ist auch möglich ein eigenes encoding zu machen mit domain knowledge
  - zb. 100 verschiedene Haus fassaden, werden auf 5 verschiedene eigenschaften gemapped, zb. Wert oder so
  - generiert hier 5 neue features, ist eine art expertenwissen einzubeziehen, aber ist natürlich sehr sensibel darauf das es korrekt ist
  - wenn wir die features von den kategorien mit ordinal machen, dann hat das dritte mit nummer 2 einen grösseren einfluss als 
  - `y = b0 + b1x1 + b2x2 ...` wenn x1 ordinal 1 ist und dann x2 ordinal 2 hat es einen grösseren einfluss, mit vektoren würde es nur einen einfluss auf das beta haben wenn es zu der kategorie gehört. Also fällen die parameter weg die sich gar nicht dazu bezeien

### Explizites Feature Engineering

- 73
  - Menge an gesammelten features, aus diesen gibt es ein neues feature, welches aber nicht in der reellen welt gesammelt wird.
  - Standard: z.b. aus der grösse des fisches können wir das volumen berechnen, das wäre dann hilfreich für das modell wenn es das gewicht schätzen will
  - In das feature engineering fliesst das expertenwissen rein, sodass man das richtige feature engineered wird
  - Man sagt dem modell teile der Kausalität sodass es es nicht alleine erraten muss. Also ein teil des learning wird abgenommen.
  - Es gibt so bessere Modelle
- Polynomielle Regression: x1, x2, (x1)^2, (x2)^2, x1*x2 (zu einem -n degree)
  - Man kann auch nur einen teil der berechnungen machen wenn sie so sinn machen, z.b. das volumen des fisches
  - Zuerst mit polynomien feature engineering, dann lienare regression, das ist ein neues modell 
- Code (poly_regression): 
  - skikit spezifisch: `sk.Pipeline` sagt, welche modelle es in welcher reihenfolge es macht
  - Jetzt hat es eine krümmung gelernt -> (x1)^2
  - Feature Space: Die features die in das modell gehen, es können variabeln drinn haben die engineert sind

### Modell evaluierung

- 79:
  - Auf den trainings daten ist es immer besser oder gleich gut
  - eher overfitting
  - Auf ungesehenen daten kann es schlechter sein, das ist ein grösseres problem
  - es kann die gewichtung eines feature vergrössern
  - das wichtigste ist jedoch ob es gut auf neuen daten ist
- 81
  - Random Forest ist am besten auf den trainingsdaten am besten, aber es ist krass overfittet auf neuen daten wird es komplett unbrauchbar sein
- 82
  - gleiche trainingsdaten, aber neue validierungsdaten andere fitts und es ist klar das random forest am schlechtesten ist
  - wird eigentlich immer auf neuen daten gerechnet, aber wenn die evaluierungsdaten sehr wenige sind oder viele ausreisser haben, kann das ergebnis verfälschen
- 86
  - Cross validation: trainings daten und dann validierungsdaten
- 87
  - Es gibt eine gefahr dass das modell durch das validierungsset nur durch zufall besser abschneidet
- 88
  - Zwei strategien -> noch ein weiteres test set
  - Oder grössere validation sets, dann aber weniger trainingsdaten...
- 89
  - train (training), 
  - validation (welches modell ist am besten), 
  - test (schauen ob es wirklich gut ist, bei daten die 0 einfluss haben), werden nie angeschaut
  - -> in der praxis am besten
  - wenn ein modell verworfen werden wegen einem test set, hat es einen einfluss auf das moell und muss weggeworfen werden und ein neues test set gesammelt werden
  - man kann auch auf ein validierungsset overfitten
- 90
  - "Herbei zaubern von test daten"
  - Wir möchten ein möglichst grosses validierungsset haben damit wir ein model nicht nur zufällig wählen
  - Einen trick das validierungsset zu vergrössern
  - K-Fold Cross Validation eine methodik
  - Alle daten sind validierung
- 91
  - 1, 2, 3, 4, 5 gelbes sind die validierungssets
  - Wir haben verschiedene arten von modellen
  - Ein model, zb. Lineare Regression trainieren sie 5 mal auf verschiedene teile vom Datenset
  - jedes model hat die validierungsdaten während dem training nicht gesehen
  - Dann werden die vorhersagen (von den verschiedenen modell arten verglichen, linear vs. polynom, etc) verglichen, wir wählen eine Modell art
  - Dann können wir die resultate der Modelle zusammen nehmen, oder eines auswählen oder wir können ein neues modell trainieren auf allen daten (keine validierungsdaten) weil wir es ja vorher validiert haben
- Hold-Out Cross validation -> train & validate
- K-fold corss validation -> gemäss 91
- 93
  - Komplex -> wie stark passt sich das model an? Wie flexibel ist es?
  - Sehr komplexes model & test schlecht -> overfitting
  - Sehr einfaches model beides nicht besonders gut -> underfitting
  - Also die komplexität richtig wählen
  - Model based view: Das model ist im zentrum, daten sind fix
  - Aber wir können nicht nur die modelle anpassen sondern auch neue daten erheben oder feature engineering
- 96
  - 1. Modell ist nicht gut es ist zu einfach, modelle sind tendenziell zu einfach
  - 2. Modell ist sehr gut auf test daten aber in der realität (neue daten) nicht. Modelle die overfittet sind, sind tendenziell komplexer
  - 3. Validierungsdaten sind schlecht
  - 4. Daten auf die seite
  - 5. Iterative über alle segmente, alles ist einmal im validierungsset, alles ist vier mal im trainings set


### Feature selection

- 98
  - nur ein teil der features werden genommen
  - macht es dem modell einfacher und das modell wird selbst einfacher
  - overfittet nicht auf zufällige features die nichts damit zu tun haben
- 101
  - grosse betas sind tendenziell eine indikation dass es overfittet ist

- 102 & 103
  - Die features müssen irgendwie standardisiert werden
  - Alle betas haben das gleiche lambda
- 102
  - L-1 Regularisierung
  - La Place verteilung
  - Wenn es ein sehr grosses beta ist, wird es durch diese kosten funktion bestraft
  - je grösser das beta desto höher die kosten funktion zusätzliche strafe in unsere kostenfunktion
  - lambda ist ein hyperparameter und wird nicht gelernt sondern von uns gesetzt. ist eine konstante
  - das modell sieht anders bei anderen lambdas
  - L-1 ist gut für feature selection, wenn ein beta nicht wichtig ist (feature) dann wird es auf null gesetzt schlussendlich
- 103: L-2 Regularisierung
  - Normalverteilung
  - betas sollten nahe 0 sind sonst wird es bestraft
  - Standardisierung ist quadratisch, wir machen wie vorher eine straffunktion dazu
  - Es wird dazu gezwungen kleinere betas zu nehmen
  - Zahlen unter 1 sind weniger streng bestraft bei L2 als bei L1
- 105
  - Standardisierung
  - Wie gross die betas werden hänge davon ab wie gross die gemessene daten sind
  - Wenn breite in meter und länge in cm dann hat die einheit einen einfluss auf die gewichtung obwohl es das nicht sollte
  - Also eigentlich die einheiten weg-werfen
- 106
  - Wenn standardisiert wird ist best practice alle einheiten losgeworden
  - z.b. wir rechnen es um so dass die werte vergleichbar sind, zum beispiel können wir grösse und geld nicht vergleichen
  - Wenn sie standardisiert werden, dann ist nur noch ein verhältnis übrig, dass nichts mehr mit den einheiten zu tun hat und sie können verglichen werden
  - die werte sind jetzt intutiv nicht mehr nachvollziehbar, die betas können auch nicht wirklich verstanden werden
  - Wenn die betas verstanden werden wollen, dann müssten wir noch viel machen, es geht nicht so einfach
  - Standard Scaler kann man als einen teil der pipeline vorstellen auch vor der regularisierung
  - Wenn standardisiert wird, muss dann der input des modells in der anwendung auch standardisiert werden
  - Achsen müssen vergleichbar werden
- 109
  - Wann standard scaler, ist wahrscheinlich keinen nachteil auch wenn wir es nicht benötigen aber die features sind dann nicht mehr interpretierbar
- 111
  - 1. Schränken die betas ein, das modell wird einfacher
  - 2. overfitting risiko weniger gross
  - 3. Einheiten entfernen sodass die achsen der features vergleichbar sind, das modell ist nicht abhängig von den einheiten der features
- 112
  - Es gibt noch weitere Hyperparameter als lambdas, zum beispiel lambdas und polynom grad
  - werden von hand gesetzt
  - z.b. wenn wir lambdas hoch machen dann müssen die betas näher bei null sein, wenn sie tief sind, ist es weniger "konfiguriert", weil das modell mehr freiheiten haben weil grosse betas nicht so stark bestraft werden
- 114
  - Manuell nach best practice ist fehleranfällig, aber wenn die rechen-kapazität nicht existiert
  - Suchen: wir versuchen verschiedene lambdas aus und dann schauen wir was am besten ist
- 115
  - Grid Search -> annhäherung, meistens nicht so gut wenn wir mehrere hyperparameter haben, weil alle kombinationen getestet werden müssen


## Tag 2

### Classification in Supervised Learning

- 123
  - Menge von vordefinierten klassen (kann auch binär sein)
- 124
  - Iris datensatz als beispiel
  - Blumenart basierend auf massen der blütenblätter
  - Output space muss keine achse sein, sondern jetzt kann es farbig sein, da es klassifikation ist
  - Kann auch durch lineare regression gemacht werden, es ist eine gerade
  - hier sind die massen standardisiert (fehler es steht im cm)

### Lineare Regression

- 125
  - Auch wenn es regression heisst ist es eine klassifikation in ML language
- 126
  - Data Specification -> Auswahl (127)
  - Model -> ist in sklearn im fit drinn
  - Kostenfunktion -> definiert
- 129
  - Wir wollen das Lineare Modell nehmen (wie Tag 1), eigentlich werte zwischen -unendlich und + unendlich
  - Die formel muss modifiziert werden zwischen 0-1 -> ist die Wahrscheinlichkeit einer klasse
  - Wird mit der Logistischen Funktion (sigmoid) gemacht (`e` -> Eulerische Zahl)
  - Die genau an 0 oder 1 eine Annäherung
  - `sigmoid(Lineare Modell)` -> zuerst nehmen wir das resultat vom modell, dann wird es in die formel rein gesetzt das ist das `z`
- 131
  - y-hat -> die klasse die wir vorhersagen, dann ist es normalerweise wenn es mehr 50 %
  - Es sind so wenige werte die genau 0 sprich 50% kommen können, dass es besser ist sich für etwas zu entscheiden als einen fehler werfen
  - können das Modell erweitern dass es einen spezifischen wert vorgeben bei dem das modell nichts vorhersagt, wenn unsicherheit wichtig ist
  - wir können auch sagen, dass wir schon bei 20% sagen dass es schon true ist, anpassung an use case
- 132
  - beta kann auch negativ sein, daher können auch bei viel variabeln kleine `z` raus kommen
- 133
  - Weil wir den output farblich dargestellt wird, können 3 features 3-Dimensional dargestellt werden
  - Ein feature wird einfach auf einer achse angezeigt -> eine linie
  - 3 features ist eine fläche
- 135
  - Metrik -> fehler für jeden einzelnen Datenpunk
  - die unterscheidung kann relevant sein in gewissen use-case
- 137
  - Die resultate vom modell
  - Können die resultat von den Trainings oder Validierungs Daten machen
  - Wenn es schon auf den Trainingsdaten fehler macht ist es vielleicht zu starr
  - Wenn es die auch auf den Validierungsdaten macht ist es vielleicht overfittet oder so
  - Gute Darstellung für die Fehler, viele kategorien gut dargestellt
  - Keine Metrik weil es immer fehler von daten punkt, es ist nicht immer aussage kräftig ob ein modell besser ist als ein anderes
- 138
  - Accuracy -> sehr einfach
    - 9 mal korrekt von 10 (9/10) -> 90 % genauigkeit
    - Wird oft verwendet weil es ist intuitiv sehr verständlich
  - wenn wir von einem fall viel mehr beispiele haben als von einem anderen ist diese aussage nicht sehr gut
- 139
  - F1-Score
    - Harmonische Mittel, Binär -> mittel von den zwei Zahlen
    - Precision -> True positive / (true positive + false positive) -> echter positive / alle guten + alle die falsch gefunden wurden
    - Recall -> True positive / (true positive + false negative) -> echte positive / alle gute + alle falschen nicht gefundenen
    - F1 ignoriert 
- 141
  - weil eine klasse so viel öfter vor kommt ist dieser score sehr hoch obwohl das modell sehr schlecht ist
- 143
  - Kostenfunktion (Maximum Likelihood)
  - Man nimmt an dass die Wahrscheinlichkeiten unabhängig sind (ist in der praxis nicht immer so), wenn man das nicht annimmt kann man nichts machen
  - hier will man einen wert möglichst nahe bei 1
  - xi (feature) sind beobachtete werte
  - Produkt symbol (wie das summen symbol)
  - Positive Sample -> eg. alles spam mail
    - Wahrscheinlichkeiten für einen datenpunkt multipliziert mit den wahrscheinlichkeiten der restlichen datenpunkten
  - Negative Sample -> eg. alle keine Spam mail
    - 1 - (Wahrscheinlichkeiten multipliziert)
  - Beide Produkte werden auch miteinander multipliziert
  - Resultat is wie wahrscheinlich sind die labels die ich habe und unserem modell
  - je mehr datenpunkte desto näher geht er gegen 0, aber wenn man sie auf dieselben daten anwenden dass sind sie trotzdem vergleichbar
  - Mit gradient descent wird negative maximum likelihood
- 145
  - Weil es auch linear ist, haben wir immernoch ein minimum
- 147
  - Gradient ascent wäre auch möglich aber es hat sich eingebürgert, dass man einfach negative gradient descent macht
- 148
  - Für die nummerische stabilität nehmen wir noch den logarithmus, ist ok, weil es das minimum nicht verändert
  - Logarithmus von multiplikation macht es daraus additionen, von der theorie kommt das selbe raus, ist im kommputer besser, weil wir haben floating points und irgendwann wird das ungenau auf dem computer, addition ist bisser addieren ist stabiler im computer
- 149
  - Softmax ist wenn wir mehr klassen haben
  - wahrscheinlichkeitsverteilung über alle klassen
  - `x` kommt ins modell -> drei wahrscheinlichkeit für alle drei klassen
  - `K` beim summenzeichen sind die anzahl klassen
  - Der term macht dass es über alle klassen loopt
  - beweis für die ite klasse -> drei modelle wie wir es vorhin hatten
    - `z1` ist ein lineares modell
    - `z2` ist ein anderes lineares modell
    - `z2` ist ein drittes lineares modell
    - Statt die sigmoid funktion um einen wert zwischen 0-1 erhalten nehmen wir eine andere funktion
  - `i` ist der index des modells und `l` ist der index der klasse
  - Predicten zwischenergebnisse -> diese individuellen ergebnisse müssen in eine Zahl umgewandelt werden weil wir wollen am schluss eine zahl
- 150
  - Es sind zwar drei lineare modelle aber sie werden zusammen trainiert weil sie über den soft max verbunden weil sie alle zusammen immer 1 geben müssen
- Code: `slides/code-examples/classification/logistic_regression.ipynb`
  - Evaluierung auf test daten sollte gut sein, weil es hat sie gesehen
- 152
  - Es ist nicht für alles geeignet wenn die daten nicht mit einer geraden von einander getrennt werden können kommt nichts sinnvolles raus
  - Vielleicht ist es möglich mit mehr features die klassen noch mehr unterscheiden
  - Feature engineering wäre möglich (polynome feature engineering (rot umrandetes bild))
- Statisk -> wir machen annahmen über eine perfekte welt, LM wir schauen was am besten passt aber wir machen keine annahmen die wir prüfen
- 156
  - 1. Fixe Gruppen, zuweisung zu einer Gruppe (im vergleich zu clustering das gruppen selbst findet)
  - 2. Annahme, dass es eine dieser Gruppe ist, wie die betas verwendet werden ist lieanr, das lineare modell ist drinn. Es hat eher eine tendenz zu underfitting. Resultat ist zwischen 0-1 (bei 2 betas), ist eine wahrscheinlichkeit.
  - 3. 


### K-Nearest Neighbour

- 165
  - Curse of Dimensionality -> Raum wir riesig bei vielen Achsen (Features), dann machen distanzen nicht mehr so viel Sinn
  - Andere Modell lernen besser die Features zu gewichten K-Nearest Neighbour kann das nicht

### Support Vector Machine

- 170
  - Binäre entscheidung
- 173
  - Kosten funktion geht nicht likelihood weil wir keine likelihood vorhersehen
- 175
  - Das ist eine Annahme um das beste modell zu wählen
  - geometrische intuition
  - geht in n dimensionen
  - position boundary und dann eine margin + / - von der position boundary, support vektoren sind die punkte die die position boundary entscheiden
- 177
  - beta ist ein vektor, der ist normiert und ist 1, sonst kann M unendlich werden
- 183
  - HARD-MARGIN -> wenn es daten gibt die nicht richtig passen, dann gibt es keine lösung
  - darum soft margin
- 184
  - bei jeder variabel darf die margin ein wenig abweichen um einen "cheat" wert (zeta)
  - aber wir müssen sagen, dass in der summe möglichst wenig verletzt werden
  - wenn C sehr gross ist dann wird cheating minimiert (stärker bestraft) und umgekehrt, das sind hyper-parameter wenn ich C kleiner mache kann die margin grösser werden und mehr datenpunkte verletzten die margin
  - Cheating darf nicht negativ sein, man darf nur in die selbe richtung cheaten nicht bei einer positiv und bei einer anderen negativ
  - Es gibt immer eine lösung, aber es kann sein, dass das cheating relativ gross sein muss um eine lösung zu haben
- 185
  - alle daten punkte verletzten der margin und sind sogar falsch klassifiziert, aber sogar die die korrekt sind, können den margin verletzten
- 186
  - beeinflusst auch den winkel der geraden da bei jeder margin das optimum anders sein kann
  - C gross, wir passen das modell stark auf die einzelnen daten and bei klein generalisieren wir es mehr
- 187
  - Keine wahrscheinlichkeit bei mehreren klassen ist nicht möglich
  - es ist möglich mehrere modelle zu trainieren kann aber schnell gross werden bei mehreren klassen
  - logistische regression ist meistens besser


### Kernel Trick

- Normal: Wir loopen über anzahl features
- Kernel Trick: wir loopen über anzahl daten

- 193
  - kann man mit jedem modell machen
  - ist vorallem effizient von support vector machine
  - in den meisten fällen wir ein indirektes feature engineering
  - feature engineering gemacht hätten. Aber es springt direkt zur lösung.
  - Es kann sein dass es rechnerisch besser ist mit kernel trick zu arbeiten als feature egineering
  - Wir wählen immernoch den Kernel, der dan die art des feature engineering macht
  - Implikation ist, dass wir nicht das neue feature "materialisieren" (explizit machen)
  - mathematisch ist es nicht ganz dasselbe alber in der praxis hier ist es gleich genug
- 194
  - Representer Theorem wird angenommen
  - weil es über alle trainingsdaten looped ist es nicht effizient bei einem grossen trainings-daten set
  - so lerne ich die alphas statt betas
  - weil die lernbare parameter ist nicht mehr von den anzahl features sondern anhand der datenpunkte abhängig
  - wenn wir mehr features als datenpunkte haben als features (das ist seltener im vergleich zu mehr daten wie features)
- 197
  - RBF kernel -> oft genommen, wie ein smoother k-nearest neighbour, weil nicht nur k nächste sondern alle datenpunkte
- Bei SVM müssen wir nur über die support vektoren loopen die einen einfluss auf das modell haben. Diese support vektoren finden wir über das erste triaining


### Decision Tree

- 199
  - ist für viele probleme sehr erfolgreich
  - ist üblicherweise binär
- 201
  - Nichts muss standardisiert werden
- 203
  - die features werden in regionen aufgeteilt, innerhalb der region wird 
  - alles was in der region ist wird tendenzeiel vorausgesagt
  - keine schräge schritte weil es dann kein baum mehr ist
  - Dann haben wir zwei neue regionen, dann kann dort nochmals unterteilt werden
  - funktioniert auch in n dimensionen
  - wo es den cut gibt wird vom learning algorithmus entschieden
  - das kann undendlich gehen, dann ist es auf dem trainings set 100%, kann aber extrem overfittet sein
  - normalerweise abbruch bedingung
  - wir zerstückeln so lange wir unreinheiten haben
  - ist auch möglich nachträglich der baum aufgeräumt wird so dass es nicht overfitting ist
  - Es macht nur einen neuen cut wenn es etwas unterteilen kann
- 204
  - dann schauen wir die wahrscheinlichkeit für eine entscheidung -> blaue region -> 100% blau
  - in einer anderen region ist es vieleicht gemischt, dann ist wahrscheinlichkeit für x gemäss anzahl von klasse und dann geteilt durch anzahl punkte in region
  - das wird für jede klasse in jeder region gemacht
  - die prediction für diese region ist die wahrscheinlichkeit x%
- 


