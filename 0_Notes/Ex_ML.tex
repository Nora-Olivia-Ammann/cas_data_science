\documentclass[a4paper, 10pt]{article}

% import packages
\usepackage[margin=0.2cm]{geometry}
\usepackage{pdflscape}
\usepackage{array}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{titlesec}
\usepackage{float}
\usepackage{needspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{graphicx}

\renewcommand{\familydefault}{\sfdefault}

\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

% global list spacing (affects all levels)
\setlist{noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt}


% now explicitly override indentation for each level you care about:
\setlist[itemize,1]{leftmargin=1.2em,label=\raisebox{0.5ex}{\scalebox{0.6}{$\bullet$}}}
\setlist[itemize,2]{leftmargin=1.4em}
\setlist[itemize,3]{leftmargin=0.5em,label=\raisebox{0.5ex}{\scalebox{0.6}{$\bullet$}}}


\setlist[enumerate,1]{leftmargin=1.2em}
\setlist[enumerate,2]{leftmargin=1.4em}
\setlist[enumerate,3]{leftmargin=0.5em}

\setlength{\columnseprule}{0.1pt}
% ------------------------------------------------------


% Configure makecell package
\renewcommand{\theadalign}{bc}
\renewcommand{\theadgape}{\Gape[4pt]}
\renewcommand{\cellgape}{\Gape[4pt]}
\renewcommand{\cellalign}{lt}

% global line spacing
\renewcommand{\baselinestretch}{1.2}

% colour definitions
\definecolor{lightergray}{gray}{0.90}

\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}}

\begin{document}
  \begin{landscape}

  
\begin{multicols}{5}

\tiny

{\footnotesize{ML}}

\begin{itemize}
    \item Modell
    \begin{itemize}
        \item Wenn für Menschen Intelligent wirkt ist KI auch wenn nicht unbedingt ML
        \item Ausprogrammiert von experten
        \item Wenn Modell gelernt aus Daten \(\rightarrow\) dann ist ML
    \end{itemize}
    \item Learning: Wissen kommt von Experten und/oder Daten
    \item Wissen
    \begin{itemize}
        \item Viel annahmen wenig daten \(\rightarrow\) viel wissen
        \item mittel wissen, mittel daten \(\rightarrow\) mischung ausgewogen
        \item wenig annahmen, viel daten \(\rightarrow\) praktisch alles daten (big player)
    \end{itemize}
    \item Lernphase: Online / Offline \(\rightarrow\) Anwendungsphase
    \item Wie? Supervised, Unsupervised, Reinforced Learning, In-Context Learning
    \item Input (unabhängige Variabel \(\rightarrow\) Output (abhängige Variabel)
\end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Linear Regression}} Supervised Regression
  \begin{itemize}
    \item \textbf{Input / Output:} Kontinuierlich
    \item \textbf{Annahme}
    \begin{itemize}
        \item 1 Lösung weil Ableitung 0
        \item Lineare Funktion
    \end{itemize}
    \item Wir haben breite von Fisch, wollen das gewicht. Formel ist unveränderlich
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Zielvariabel festlegen
        \item features selection
        \item Encoding Kategorisch
        \item Standardising Nummerisch
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item Gewichtete Summe von Input variabeln (features) und Gewichten \(\beta\)
        \item Das Lineare Modell ist nicht zwingend eine Gerade kann auch fläche sein\(\rightarrow\) Hyper-Ebene.
        \item Formel \(\hat{y} = \sum\limits_{i=1}^{M}x_i\beta_i + \beta_0\)
        \item Fixe ausprogrammierte Formel
        \item Modell lernt die \(\beta\)
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item Residuen \(y- \hat{y} = \epsilon\) (zusammenfassing von \(\epsilon\) ist Metrik)
        \item Mean Absolute Error or Mean Sqare Error
        \item Gibt auch custom metriken, ist problemabhängig (wie mit ausreisser umgehen?, etc)
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item In Linear Regression kann auch MSE sein
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item Gradient Descent
        \item Analytische Methode
    \end{itemize}
    \item \textbf{Pro} niedrige gefahr für overfitting
    \item \textbf{Con} kann sich nicht gut an daten anpassen (starr), kann keine Krümmungen lernen
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Logistic Regression}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item Ziel variabel ist eine der vorgegebenen Gruppen
        \item \(\beta\) sind Linear
        \item hat eine tendenz zum underfitting
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Kategorische vorgegebene Zielvariabel
        \item Features werden gewählt
        \item Kategorische Features Encoden
        \item Wenn Regularisiert (L1/L2) \(\rightarrow\) Standardisiert
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item Modifikation Lineares Modell
        \item Output zwischen 0-1 (wahrscheinlichkeit für eine Klasse)
        \item Eg. Sigmpoid / Logistik  \(\rightarrow\) resultat von Modell in diese Funktion (z) einsetzten
        \item \(\phi(z) = \dfrac{1}{1 + e^-z}\) (definition von funktion Z)
        \item \(p(y=1|x_i) = \phi(\beta_0 + x_i\beta_1)\) (Wahrscheinlichkeit klasse y)
        \item \(p(y=0|x_i) = 1- \phi(\beta_0 + x_i\beta_1)\) (Wahrscheinlichkeit andere klasse als y)
        \item \(\hat{y} = arg \; max  \;  p(y = k | x_1)\) (maximum davon \(\rightarrow\) predicted klasse)
        \item Kann auch konfiguriert werden dass es schon ab eg. 20\% Klasse 1 predicted
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item Was sind für Fälle möglich pro Datenpunkt
        \item Accuracy (nicht gut für inbalanced)
        \item F1-Score (gut bei inbalanced)
        \item Confusion Matrix keine Metrik und nicht immer aussagekräftig
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item Maximum Likelihood
        \item Sigmoid \(\phi\): Beweiss für eine Klasse
        \item Softmax: Beweiss für mehrere Klassen
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item Gradient Descent auf Max Likelihood anwenden
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{K-Nearest Neighbours}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item Wir lernen keine \(\beta\)
        \item Wir suchen die nächsten k Punkte im train set (basierend auf mehrzahl)
        \item k ist Hyperparameter
        \item Keine Optimierung, keine Kostenfunktion
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Vorgabe: Was ist ziel variabel, was sind features?
        \item Encode Kategorsiche Features
        \item Standardisie Nummerische Features
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item \textbf{Curse of Dimensionality}
        \begin{itemize}
            \item Nicht gut bei vielen Features
            \item im hoch dimensionalen raum bedeutet distanz nicht viel
        \end{itemize}
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Support Vector Machine}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item Support Vektoren: Punkte die die Decision Boundary Entscheiden (kleine Anzahl)
        \item Hoch dimensional möglich
        \item Keine wahrscheinlichkeit bei mehreren klassen ist nicht möglich
        \item es ist möglich mehrere modelle zu trainieren kann aber schnell gross werden bei mehreren klassen
        \item logistische regression ist meistens besser
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Vorgegebene kategorische Zielvariabel
        \item Feature selection
        \item Encode Kategorische
        \item Standardisier Nummerische
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item Heaviside Step Function (TODO: graph)
        \item Binäre Entscheidung, keine Wahrscheinlichkeit
        \item Decision Boundary ist linie mit maximalen Abstand zum nächsten Punkt der anderen Klasse
        \item Geometrische Annahmen
        \item Soft margin (bei hard margin gäbe es daten bei denen es keine lösung gibt)
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item wenn C sehr gross ist dann wird cheating minimiert (stärker bestraft) und umgekehrt, das sind hyper-parameter 
        \item wenn ich C kleiner mache kann die margin grösser werden und mehr datenpunkte verletzten die margin
        \item Cheating darf nicht negativ sein, man darf nur in die selbe richtung cheaten nicht bei einer positiv und bei einer anderen negativ
        \item Es gibt immer eine lösung, aber es kann sein, dass das cheating relativ gross sein muss um eine lösung zu haben
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item bei jeder variabel darf die margin ein wenig abweichen um einen "cheat" wert (zeta)
        \item aber wir müssen sagen, dass in der summe möglichst wenig verletzt werden
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Decision Trees}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item Kann keine extrapolation machen für Daten ausserhalb der Trainingsdaten
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}
{\scriptsize{Clustering}} Unsupervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item Zusammenhänge in Datenwolken finden
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item eg. k-means algorithmus
        \item Anzahl k festlegen
        \item sucht minimale distanz
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Principal Component Analysis}} Unsupervised Dimensionality Reduction
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Welche Features haben wir? eg. Pixel
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Non-Negative Matrix Factorisation}} Unsupervised Dimensionality Reduction
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Auto Encoder}} Unsupervised Dimensionality Reduction
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\scriptsize{Fully Connected Neural Network}} Typ: ???
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\end{multicols}

\newpage

\begin{multicols}{5}

\tiny

{\footnotesize{Optimisation}}

\begin{itemize}
    \item Optimisation wird von der Kostenfunktion geleitet.
    \item Mechanismus wie wir die \(\beta\) (lernbare parameter) eines Modells für eine \textbf{Kostenfunktion} aus \textbf{Daten lernen}.
    \item unterschiedliche mit anderen eigenschaften / präferenzen  \(\rightarrow\) Performanz (Zeit), Genearlisierung (Qualität)
    \item Wenn visualisierung von optimalen \(\beta\) eine Schüssel \(\rightarrow\) Convex Kostenfunktion
\end{itemize}


\vspace{0.4em}
{\scriptsize{Optimisation Algorithm}}
      \begin{itemize}
      \item {\textbf{Analytische Methode}}
      \begin{itemize}
          \item nicht iterativ (löst die Normalgleichung)
          \item besser bei wenigen features
          \item Matrix muss invertierbar sein
      \end{itemize}
      \item \textbf{Gradient Descent}
      \begin{itemize}
          \item Iterativ, Start zufällige \(\beta\)
          \item Ableitung der Kostenfunktion
          \item mit jedem Schritt in die bessere richtung, bis zum minimum
          \item gut für viele Daten und \(\beta\)
      \end{itemize}
    \end{itemize}


\vspace{0.4em}
\hrule
\vspace{0.7em}


{\scriptsize{Metrik: Regression}}
\begin{itemize}
  \item \textbf{Mean Absolute Error (MAE)}
  \begin{itemize}
      \item \(\frac{1}{n}\sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|\)
      \item robust gegen ausreisser
      \item Fehler $>$ 1 start bestraft
      \item Fehler $<$ 1 schwächer bestraft
  \end{itemize}
\item \textbf{Mean Square Error (MSE)}
  \begin{itemize}
      \item \(\frac{1}{n}\sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2\)
      \item beieinflusst von Ausreisser
      \item bestraft grosse Fehler stark
  \end{itemize}
\end{itemize}

\vspace{0.4em}
{\scriptsize{Metrik: Klassifikation}}

\begin{itemize}
    \item True Positive \(\rightarrow\) TP / True Negative \(\rightarrow\) TN, etc
    \item \textbf{Accuracy}
      \begin{itemize}
            \item \(\dfrac{TP + TN}{TP + TN + FP + FN}\)
            \item irreführend bei inbalanced klassen
            \item 99 A und 1 B, wenn immer A dann gute accuracy auch wenn Modell dumm
      \end{itemize}
    \item \textbf{Precision}
        \begin{itemize}
            \item \(\dfrac{TP}{TP + FN}\)
        \end{itemize}
    \item \textbf{Recall}
    \begin{itemize}
        \item \(\dfrac{TP}{TP + FN}\)
  \end{itemize}
  \item \textbf{F1-Score}
    \begin{itemize}
        \item \(F_1 = 2 \cdot \dfrac{Precision \cdot Recall}{Precision + Recall}\)
        \item gut bei inbalanced klassen
    \end{itemize}
\end{itemize}


TODO: Bestimmtheitsmass \(R^2\)


\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize{Kostenfunktion}}

\begin{itemize}
    \item Guides Optimisation
    \item Minimise errors, improve model accuracy, used during training
\end{itemize}


{\scriptsize{Maximum Likelihood}}
\begin{itemize} 
    \item TODO: furuther investigation
        \item we wish to maximize the conditional probability of observing the data (X) given a specific probability distribution and its parameters
        \item \( p(\overrightarrow{y} | X ) = \prod \phi(x^i\beta) \cdot \prod{1-\phi(x^i\beta)}\) \\
        probability feature \(x_i \rightarrow\) +ve sample * -ve Sample 
        \item Wert möglichst nahe 1 \(\rightarrow\) Wahrscheinlichkeit für eine Klasse
        \item Resultat: Wie wahrschienlich sind die labels die ich habe und unserem Modell
        \item Je mehr Datenpunkte desto näher bei 0, aber immernoch vergleichbar
    \end{itemize}




\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize{Feature Preprocessing}}

{\scriptsize{Encoding}}

\begin{itemize}
    \item Information in anderer Maschienen freundlichen Art darzustellen.
    \item oft notwendig für text
\item mit oder ohne Informationsverlust

\item \textbf{Kategorisch}
\begin{itemize}
    \item \textbf{Ordinal}
    \begin{itemize}
        \item mapping von kategorie mit aufsteigender Zahl 0-n
        \item 1 Feature \(\rightarrow\) 1 Feature
        \item hat eine Ordnung, grösse der Zahl hat einen Einfluss!
        \item nicht gut wenn es keine Ordnung hat oder Abstände nicht regelmässig
        \item eg. Kleidergrössen xs-xl (0-4) könnte passen weil es hat eine inherente bedeutung
    \end{itemize}
    \item \textbf{One-Hot}
    \begin{itemize}
        \item n-Einzigartige Eigenschaften \(\rightarrow\) n-Dimensionalen Vektoren
        \item 1 Feature \(\rightarrow\) n Feature
        \item Vektor, 0/1, 1 eintrag wo es in der reihenfolge stimmt, sonst 0
    \end{itemize}
    \item Meistens one-hot da ordnung nichts bedeutet
    \item Domänenwissen kann einfliessen zum eg. neue Kategorien zu bilden
\end{itemize}

\item Meistens One-Hot weil Dinge keine Ordnung haben
\item Möglich mit Domänen-Wissen eigene Encoding machen (eg. Kategorien zusammenfassen)
\end{itemize}

\vspace{0.4em}
{\scriptsize{Feature Selection}}

\begin{itemize}
    \item Auswahl von Features die relevant sind, einfacher für das Modell
    \item Ansätze
    \begin{itemize}
        \item Manuell mit Domänenwissen (schwierig)
        \item Automatisch Ausschliessen von features die nichts mit der Zielvariabel zu tun haben
    \end{itemize}
\end{itemize}

\vspace{0.4em}
{\scriptsize{Standardise}}
\begin{itemize}
    \item \texttt{StandardScaler} = \(\dfrac{Wert - Durchschnitt}{Standardabweichung}\)
    \item Schiebt alles um 0 Punkt
    \item Entfernt Einheiten, separat pro Feature, erhält Verhältnisse
    \item notwendiger Schritt sodass Modelle richtig funktionieren
    \item Wann?: Parameter müssen vergleichbar sein
    \item Wann?: Distanzen im input space spielen eine Rolle, ansonsten haben grosse Werte einen einfluss
\end{itemize}

\vspace{0.4em}
{\scriptsize{Dimensionality Reduction}}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize{Feature Engineering}}

{\scriptsize{Explizit}}

\begin{itemize}
    \item Bestehende features \(\rightarrow\) neue features
\item expertenwissen, nimmt einen Teil des Learning Ab \(\rightarrow\) gibt wissen vor.

\item {\textbf{Polynomielle Regression}} (Standard)
\begin{itemize}
    \item Input Space: \texttt{x}
    \item Feature Engineering \(\rightarrow\) Feature Space: \texttt{x}, \texttt{$x^2$}
    \item Modell \(\rightarrow\) Output Space: \texttt{y}
\end{itemize}

\item E.g. Volumen von Fisch berechnen, so kann regression Polynome haben
\item Sind besser auf trainings daten, nicht immer besser auf test daten
\end{itemize}

\vspace{0.4em}
{\scriptsize{Kernel Trick}}

\vspace{0.4em}
\hrule
\vspace{0.7em}


{\footnotesize{Modell Komplexität}}

{\scriptsize{Overfitting vs. Underfitting}}

\begin{itemize}
    \item \textbf{Overfitting}: super training, schelcht test
    \item \textbf{Underfitting}: schlecht training schlecht test
    \item Je komplexer das Modell desto höher die Gefahr für overfitting
\end{itemize}

\vspace{0.4em}
{\scriptsize{Cross Validation}}

\begin{itemize}
    \item vergleich von verschiedenen ML Modell-Arten
    \item Gefahr: falsches Modell, nur durch Zufall gut, mehr validierungsdaten, desto unwahrscheinlicher
    \item 2 Strategien
    \begin{itemize}
        \item Train, Val, Test (no peeking)
        \item k-Fold mehr validierungsdaten herbeizaubern
    \end{itemize}
    \item Alle Daten sind im validierungsset
    \item Iterativ üver alle Segmente gehen
    \item Mehr rechenzeit, aber weil wenig daten nicht ein problem
\end{itemize}

\vspace{0.4em}
{\scriptsize{Regularisation}}
\begin{itemize}
    \item Annahme: Grosse \(\beta\) zeichen für overfitting, sollten nahe 0 sein, Modell wird einfacher
    \item Daten müssen Standard Skaliert werden
    \item Regularisierungsstärke \(\lambda \rightarrow\) ist Hyperparameter
    \item Grosses \(\lambda \rightarrow\) stark konfiguriert \(\beta\) näher bei 0
    \item {\textbf{L1 Lasso}}: Absolut \(|\beta|\)
    \begin{itemize}
        \item Grosse \(|\beta|\) werden bestraft
        \item Gut für automatische Feature-Selection (unwichtiges \(|\beta|\) verschwindet)
        \item \(J_{\text{Reg}}(\beta) = J(\beta) + \lambda \sum_{j=1}^{p} |\beta_j|\)
    \end{itemize}
        \item {\textbf{L2 Ridge}}: Quardrat \(\beta^2\)
    \begin{itemize}
        \item \(|\beta|\) nahe 0 sonst bestraft
        \item Zahlen unter 1 weniger bestraft als bei L1
        \item Alle \(|\beta|\) klein, ausgewogen in vergleich zu L1
        \item \(J_{\text{Reg}}(\beta) = J(\beta) + \lambda \sum_{j=1}^{p} \beta^2_j\)
    \end{itemize}
\end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}


{\footnotesize{Model Selection}}

{\scriptsize{Hyper Parameter}}
\begin{itemize}
    \item Werden nicht gelernt \(\rightarrow\) konfiguriert das Modell
    \item Z.B. Regularisierungsstärke
    \item Manuell: Erfahrung \& Theorie  \(\rightarrow\) schwierig
    \item Suchen: Ausprobieren und auf ungesehen Daten merken  \(\rightarrow\) Rechenintensiv
    \begin{itemize}
        \item \textbf{Grid Search:} Liste von Werten, ausprobieren, nicht gut bei vielen hyper Parameter wegen kombination
        \item \textbf{Randomised Search:} Zufällige Zahlen innerhalb eines vorgegebenen Bereiches
    \end{itemize}
\end{itemize}


\vspace{0.4em}
{\scriptsize{Algorithm Selection}}

\vspace{0.4em}
\hrule
\vspace{0.7em}


\end{multicols}


\newpage

\begin{multicols}{5}

\tiny

{\footnotesize{General}}
\begin{itemize}
    \item \textbf{Logistische Reg vs. SVM?}
    \begin{itemize}
        \item Log Reg: wahrscheinlichkeit für klasse mit schwellenwert
        \item SVM: binäre Trennlinie mit maximalen Abstand
    \end{itemize}
    \item \textbf{Zusammenhang SVM \& Lineare Reg}
    \begin{itemize}
        \item minimiert distanz
        \item lineare decision boundary
    \end{itemize}
\end{itemize}

\vspace{1em}
{\footnotesize{Wie Problem Angehen?}}
\begin{itemize}
    \item \textbf{Problem Verstehen:} Klassifikation, Regression, etc.?
    \item \textbf{Ziel Verstehen:} Einfluss auf Metrik Auswahl
    \item \textbf{Verstehe / Evaluiere Daten:} genug? Garbage-in Grabage-out
    \item \textbf{Datenlandschaft Evaluieren:} crawlable, API, etc.?
    \item \textbf{Domänen Experten:} Was ist wichtig (Feature Selection). Bekannte Zusammenhänge (feature engineering)
    \item \textbf{Datenqualität:}
    \begin{itemize}
        \item verstehen
        \item Standardisieren, Encoden
        \item Inkonsitenz finden \(\rightarrow\) erklären oder ausschliessen
        \item Visualisieren \& Analysieren
        \item Muster mit Domänen Experten bestätigen
        \item Zusammenhänge überprüfen
    \end{itemize}
\end{itemize}

\vspace{0.4em}




\end{multicols}

\end{landscape}
\end{document}