# Data Mining Notes

- 3 -> Definition
- 7: 
  - Model -> Decision Tree für Vorhersage um zu schauen ob eg. es sich lohnt einer Person Werbung zu schicken
  - 2 Teile, Modellieren dann in Produktion
- 9 (älteres Format)
  - Business Understanding -> Was wollen wir machen? Bilden Fragen
  - Data Understanding -> Geben die Daten die wir schon haben die Info her um die Fragen zu beantworten
  - Data Preparation -> Sind die Daten so wie wir denken (data wrangling)
  - Modelling -> Die Daten in ein Modell zu bringen, das dem Business Understanding hilft
  - Evaluation -> wie gut funktioniert es? Man hat metriken um zu schauen ob es gut ist
    - gibt dem Business zurück, visualisieren, etc. was ist passiert.
    - Kann sein dass es follow up gibt von dem Business Fachpersonen, und der Prozess beginnt von neuem
  -  Deployment -> wenn das Modell gut ist und es keine weiteren Fragen mehr gibt
-  16:
   -  Wenn zwei Vektoren aneinander gelegt werden, kann herausgefunden werden wie gross der Winkel dazwischen ist.
   -  Wenn zwei Vektoren in die gleiche Richtung zeigen -> 1, entgegengesetzt -> 1, rechtwiklig -> 0
-  19: Clustering
   -  Aufteilen in Gruppen die in irgendeiner Form ähnlich sind.
- 21:
  - Y unterschiedliche Datensätzte , das farbige ist was der algorythmus als ähnlich bezeichnen würde.
-  22: K-means algorithm
   -  **K** -> anzahl der punkte nach denen geclustert wird, wird irgendwie fest gelegt werden. Kann fix sein, eg.
      Kleidergrössen.
   -  Startpunkt komplett zufällig, der punkt bei dem alle punkte die dem ähnlich sind werden zugeordnet
   -  Dann werden mehrere Punkte hinzugefügt.
   -  Dann ortet man die punkte nach abstand, nimmt einen neuen zentroid beim mittelpunkt, und fügt neue punkte hinzu
   -  Es kann sein, dass die Punkte die Farbe wechseln, weil sie einem neuen Zentroid hinzugeordnet werden
   -  optimiert sich über die zeit, irgendwann bewegen die zentroiden sich nicht mehr. Dann kann gestoppt werden
   -  konvergiert nicht jedesmal zur selben lösung, hängt von der start position ab. Daher wird es meistens mehrmals laufen gelassen
-  24
   -  Idealerweise will man ein möglichst kleines K haben.
   -  Wie der quadrierte Abstand sich zu den Zentroiden verhält
   -  Je mehr cluster wir starten (k) desto kleiner ist die summer der quadratischen Distanzen
   -  Hier wenn wir bei K=3 nimmt der abstand nicht gross ab, also ist das ein erstes optimum.
- 26: DBSCAN
  - epsilon -> distanz zwischen zwei samples, max distanz um reachable zu sein
  - anzahl minimal points, wie viele punkte gegeben sein müssen um ein punkt als kern punkt zu zeichnen
  - Border points: erreichbar von den core points, aber selbst haben sie nicht genügend nachbarn um ein core point sein können.
  - Noise points: sind zu weit weg von irgendeinem punkt um reachable zu sein.
  - Als erstes schauen wir nur wo core points sein könnten. Wenn im epsilon radius keine eg. 4 punkte drinn sind, ist es kein core point.
  - Border points sind auch innerhalb einer cluster
  - Expansion, startet mit einem punkt und markiert alle punkte die innerhalb vom epsilon sind
  - Stärke ist, dass automatisch implizit über die definition von epsilon und min points outliers identifiziert werden können
  - Ziel: möchte eine einordnung machen um daten zusammen zufassen. Wie weit muss der Datensatz reduziert werden?
  - Werte Wahl sind abhängig von dichte von datensatz. Bei dichtem datensatz kann ein kleines epsilon gewählt werden.
  - Algorithmus ist deterministisch, das heist egal wie oft es läuft wenn die reihenfolge konsistent ist, werden die randpunkte immer den gleichen cluster zugeordnet. Wenn die Reihenfolge randomisiert wird, dann werden vielleicht die randpunkte anderen cluster zugeordnet. Daher wenn mann diese sachen robust machen will muss es mehrmals randomised ausgeführt werden.
- 28: Co-Occurrence Grouping (market-basekt)
  - z.b. Wie hoch sind die chancen dass sachen zusammen gekauft werden? (Velo, hohe chance, dass ein velohelm dabei ist)
  - Basierend darauf kann eine similarity gebaut werden.
- 29: Classification
  - Zuordnung von items
  - wir schauen Nearest Neighbour Methode
- 31: k-Nearest Neighbour (kNN)
  - Wieviele Nachbarn schaue ich an um meinen punkt zuzuordnen
  - Welches ist die dominierende klasse in meiner nachbarschaft
  - anhand von "k" kann ich sagen zu welcher klasse es gehört und kann ihn entsprechend zuordnen
  - Clustering, unsupervised nach klassen suchen, weiss nicht welches, wieviele etc.
  - kNN, ich kenne die klassen und will die Datenpunkte denen zuordnen.
  - Man muss daten haben bei denen man die klasse weiss (ground truth)
  - Mit diesem bekannten Datensatz wird gearbeitet
  - Hier der Punkt am fragezeichen wird rot zugeordnet weil die meisten darum sind rot. Ob das ergebniss stimmt wissen wir nicht
  - DataSet split:
    - wir ein daten set brauchen um ein modell zu trainieren, dann können wir nachher nicht kontrollieren wie gut das Modell ist, weil alle verfügbare daten gebraucht wurden um das modell zu trainieren.
    - Daher das datenset splitten, sodass wir überprüfen wie gut das modell ist
    - Wenn wir 100% accuracy haben, dann ist das modell overfittet und es hat den trainingsdaten satz auswendig gelernt
    - Das kann nicht nur bei kNN passieren sondern auch bei anderen. Daher müssen die Daten aufgeteilt werden.
    - Duplikate entfernen ist wichtig weil sonst over fitting. In der Realität sind duplikate nicht echte, also ausversehen doppelt eingegeben und nicht echte duplikate die zwei separate events beschreiben.
    - Die Typische aufteilung ist ungf. 
    - 70% -> Training: Daten zum trainieren
    - 20% -> Validation Data: Welche von meinen hyper parameter sind gut?  -> Optimierung von Hyper parameter. Ist mit diesem Set optimiert also ist auch im modell miteinbezogen. Hyperparameter ist kein wert der in den daten steckt sondern zum beispiel der "k" wert, der verändert werden kann. Dieser wird verändert bis das Modell am besten läuft.
    - 10% -> Test Set: wenn wir z.b. drei modelle gegeneinander test dann brauchen wir das set, weil das war noch komplett unberührt und kein modell weiss etwas davon. Anhand der resultat kann gesagt werden welches modell am besten war. Aber das resultat wird nicht verwendet um weiter an den modellen zu arbeiten.
- 33: Association analysis
  - Aus zwei faktoren folgt ein drittes
  - eg. Patienten die astma haben und hochen blutdruck haben ein erhöhtes x
  - Ein faktor alleine ist nicht aussage kräftig aber die kombination ist aussage kräftig
- 34: Regression
  - prediction
- 35: Profiling
  - Typisches Nutzerverhalten
  - In Gruppen unterteilen, kann vorher klassifizierung machen
- 36: Link Prediction
  - Bereich Social Network interessant
  - geht darum dass man versucht neue kontakte vorzuschlagen
  - Wer könnte interessant sein
  - eg. eine Gruppe hat viele links untereinander, es kommt eine neue person dazu, tritt vielleicht in die Gruppe ein?
  - Citation links: eine publikation wird von einer anderen zitiert... welche publikationen sind da spannend?
- 37: Data Reduction
  - Wird oft versucht daten zu reduzieren. Also ein sub-sample nehmen
  - Representation Learning -> Sucht ein sub-sample dass das original möglichst gut räpresentiert
  - Data Reduction kann entweder die Anzahl daten reduzieren oder einfach daten punkte (spalten) wegwerfen
- 38: Dimensionality Reduction
  - grosse anzahl von dimensionen (multi-dimensiale vektoren) herunterbrechen auf weniger dimensionen, optimal 2-3 weil wir das gut darstellen und verstehen können
  - Bild: Projektion, etwas dass in 3-D existiert und wird in einem schatten an die want, 3. Dimension verloren
  - Welche Dimension kann ich weg-lassen sodass die Daten nachher immernoch ausagekräftig ist
  - Vorteil: Zusammenhänge die nicht sichtbar waren, weile es zu kompliziert war, jetzt sichbar sind.
- 40: Welche Dimension hat die meiste Varianz, suche die rechtwinklige dazu die die meiste Varianz hat. Dann weiss man welches die Hauptachsen sind. Dann wird der Datensatz so gedreht dass wir rechtwinklich auf diese Ebene schauen
  - es erhält die distanzen aller punkte zu einander
  - x, y, z sind immer neue angaben, ausser der datensatz war schon von anfang an 2-Dimensional
  - Die meiste Varianz ist in den 1-5 Dimensionen schon erhalten, und man kann die weiteren Dimensionen ignorieren
- 41: 
  - nicht linearer ansatz der mächtig ist, aber sehr kompliziert ist
  - Zei Schritte
  - 1. Hoch dimensionalen raum (eg. 3)
    - Eine verknüpfung zwischen einem datenpunkt und dem nachbarn
    - Die stärke von der verknüpfung hängt von der distanz ab
  - 2. Wir schauen immer zwei an. Entweder die dicht zusammen sind oder weit auseinander und passe es der anspannung an.
    - Wenn die punkte dicht aneinander waren, wollen sie nachher auch dicht bei einander sein
    - optimiert das iterativ
  - 42: je mehr nachbarn gewählt werden desto näher sind sie bei einander
    - extrem rechenintensiv
    - n_neighbors sorgt dafür, je grösser das wird desto mehr wird auf die nachbarschaft geschaut. Mehr von der struktur rund herum wird erhalten. Je kleiner desto mehr wird jeder Punkt mit sich selbst vergleicht
    - Min_distance -> wie dicht nach der projektion die punkte zusammen rutschen lässt. Mit wieviel abstand von einander die gummi bänder schon entspannt sind. Min dist 0, die gummi bänder sind nur entspannt wenn die punkte aufeinander liegen. Höher, die punkte müssen nicht aufeinander liegen bis die gummi bänder entspannt sind.
    - Ist verlust behaftet, das heisst es kann sein, dass gewisse information wirklich nicht räpresentiert ist.
  - Random Seed: zufallszahlen generieren können computer eigentlich nicht generieren. Computer rechnen pseudo zufallszahlen. Ist ein mathematischer algorithmus der Zahlen rausgibt die so nah wie möglich an einem wirklichen zufall ist. Wenn immer mit derselben zahl startet kommt dasselbe raus. Das heisst es kann nicht immer mit der selben zahl zu starten. Oft gestartet mit uhrzeit. Start-Zahl ist "seed" wenn random seed steht, dann sind es jedes mal andere ergebnisse. Wenn der seed gesetzt wird, dann kommen immer dieselben zahlen raus.
- Umap -> Exercises
  - Dimensionen die UMAP benutzt hat um die projektionen zu machen
  - Was bedeuten die nummern auf der x und y achse? Nichts direktes, ist ein mass dafür, wie die punkte noch zusammen sind nachdem die berechnung ist. Es kann nichts wirklich damit zu verrechnen.
  - Die Farben sind nach der projektion ziemlich getrennt und konsistent. Spricht die zusammenhänge sind erhalten.
  - Z.b. es gibt zwei flecken die 2 Parkplätze haben, aber es sind immernoch zusammen
  - die dinge liegen nicht direkt nebeneinander auch wenn sie in einer spezifischen dimension identisch sind, weil die anderen parameter auf miteinbezogen werden.
  - Dann kann man sich einen cluster anschauen und ansehen wie sie zusammenhängen.
  - BuildingArea innerhalb einem cluster ist es plötzlich divers, YearBuilt auch nicht mehr homogen.
  - Man sieht dass es fast keine brüche hat in den daten hat sondern es immer ein fächer ist, mit einem gradient
  - Typ ist information die wir nicht hineingegeben haben. Spannend ist, dass es aus den anderen variabeln zusammenhänge habt die mit dem gebäude typ zusammenhängen, weil es hat farb gradiente es hat eine gewisse trennung
  - Die grauen sind ziemlich ähnlich wie die häuser, weil die dimensionen die gewählt wurden zwischen townhouse und haus ähnlich sind.
  - Subburb, ist konfetti. Also die dimensionen sind nicht aussage kräftig. Die ähnlichkeit ist weniger da.

# Prüfungsfrage

Warum wird ein Train-Validation-Test-Split bevorzugt gegenüber einer einzigen Datensammlung für Training und Test?
    A. Weil es die Modellkomplexität verringert
    B. Weil es sicherstellt, dass das Modell mit einer höheren Lerngeschwindigkeit trainiert
    C. Weil es die Verfügbarkeit von separaten Daten für Training, Optimierung und abschließende Bewertung ermöglicht
    D. Weil es verhindert, dass das Modell während des Trainings Fehler macht


