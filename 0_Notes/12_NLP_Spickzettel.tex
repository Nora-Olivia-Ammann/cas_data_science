\documentclass[a4paper, 10pt]{article}

% import packages
\usepackage[margin=0.2cm]{geometry}
\usepackage{pdflscape}
\usepackage{array}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{titlesec}
\usepackage{float}
\usepackage{needspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage{booktabs}   % for \toprule, \midrule, \bottomrule
\usepackage{adjustbox}  % for \begin{adjustbox}{max width=\textwidth}
\usepackage{etoolbox}
\AtBeginEnvironment{tabular}{\tiny}

\renewcommand{\familydefault}{\sfdefault}

\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

% global list spacing (affects all levels)
\setlist{noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt}

% now explicitly override indentation for each level you care about:
\setlist[itemize,1]{leftmargin=1.8em}
\setlist[itemize,2]{leftmargin=1.4em}
\setlist[itemize,3]{leftmargin=0.5em}

% Only change level 1 itemize bullets
\setlist[itemize,1]{label=\raisebox{0.5ex}{\scalebox{0.6}{$\bullet$}}}
%\setlist[itemize,3]{label=\raisebox{0.5ex}{\scalebox{0.6}{$\bullet$}}}

\setlength{\columnseprule}{0.1pt}
% ------------------------------------------------------


% Configure makecell package
\renewcommand{\theadalign}{bc}
\renewcommand{\theadgape}{\Gape[4pt]}
\renewcommand{\cellgape}{\Gape[4pt]}
\renewcommand{\cellalign}{lt}

% global line spacing
\renewcommand{\baselinestretch}{1.2}

% colour definitions
\definecolor{lightergray}{gray}{0.90}

\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}}

\begin{document}
  \begin{landscape}

\tiny

\begin{itemize}
    \item \texttt{?} \(\rightarrow\) 0-1
    \item \texttt{*} \(\rightarrow\)  0-n
    \item \texttt{+} \(\rightarrow\) 1-n
    \item \texttt{*?} \(\rightarrow\) 0-n (lazy)
\end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize TF-IDF}
\begin{itemize}
    \item 
\end{itemize}

{\scriptsize{Term Frequency TF}}
\(\uparrow\) weight of common words in doc \\
tf(term,doc) = \# times term in doc / total \# of terms in doc

\vspace{-12pt}
\begin{table}[ht]
\begin{adjustbox}{max width=\textwidth}
\setlength{\tabcolsep}{1pt}      % default is ~6pt
\renewcommand{\arraystretch}{0.5} % default is 1.0
\begin{tabular}{l|r|r|r|r|r|r|r|r|r}
\midrule
\textbf{sentence} 
& \textbf{a} & \textbf{cat} & \textbf{dog} & \textbf{is} & \textbf{it} 
& \textbf{my} & \textbf{not} & \textbf{old} & \textbf{wolf} \\
\midrule
\textit{``It is a dog.''} & 0.25 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0 & 0 \\
\textit{``my cat is old''} & 0 &  0.25 & 0 & 0.25 & 0 & 0.25 & 0 & 0.25 & 0 \\
\textit{``It is not a dog, it a is wolf.''} & 0.22 & 0 & 0.11 & 0.22 & 0.22 & 0 & 0.11 & 0 & 0.11 \\
\midrule
\end{tabular}
\end{adjustbox}
\end{table}
\vspace{-11pt}

{\scriptsize{Inverse Document Frequency IDF}}
\(\downarrow\) weight for common words \(\uparrow\) weights for rare words

idf(term) = log(\# docs / \# docs with term)

\vspace{-12pt}
\begin{table}[ht]
\begin{adjustbox}{max width=\textwidth}
\setlength{\tabcolsep}{1pt}      % default is ~6pt
\renewcommand{\arraystretch}{0.5} % default is 1.0
\begin{tabular}{l|l}
\midrule
\textbf{term} & \textbf{idf} \\
\midrule
a & log(3/2) = 0.17 \\
cat & log(3/1) = 0.47 \\
dog & log(3/2) = 0.17 \\
is & log(3/3) = 0 \\
my & log(3/1) = 0.47 \\
.. & .. \\
\end{tabular}
\end{adjustbox}
\end{table}
\vspace{-11pt}

{\scriptsize{TF-IDF}} tf-idf(term,doc) = tf(term,doc) * idf(term) 

{\scriptsize{Query}} 
\textbf{"cat is wolf"} 
If one words appears eg. 2 times in query, score is multiplied by 2
\vspace{-12pt}
\begin{table}[ht]
\begin{adjustbox}{max width=\textwidth}
\setlength{\tabcolsep}{1pt}      % default is ~6pt
\renewcommand{\arraystretch}{0.5} % default is 1.0
\begin{tabular}{l|r|r|r|r|r|r|r|r|r||rr}
\midrule
\textbf{sentence} 
& \textbf{a} & \textbf{cat} & \textbf{dog} & \textbf{is} & \textbf{it} 
& \textbf{my} & \textbf{not} & \textbf{old} & \textbf{wolf} 
& \textbf{score} & \textbf{rank} \\
\midrule
\textit{``It is a dog.''} 
& 0.044 &  & 0.044 & \cellcolor{yellow} 0 &  0.044 &  &  &  & & 0 & - \\
\textit{``my cat is old''} 
& & \cellcolor{yellow} 0.119 & & \cellcolor{yellow} 0 & & 0.199 &  & 0.119 & & 0.119 & 1 \\
\textit{``It is not a dog, it a is wolf.''} 
& 0.039 &  & 0.02 & \cellcolor{yellow} 0 & 0.039 &  & 0.053 &  & \cellcolor{yellow} 0.053 & 0.053 &  2 \\
\midrule
\end{tabular}
\end{adjustbox}
\end{table}
\vspace{-11pt}


\vspace{0.4em}
\hrule
\vspace{0.7em}


{\footnotesize Attention (Transformer Core)}

\begin{itemize}
    \item \textbf{which parts of a sequence matter most} for each token
    \item Instead of reading strictly left-to-right, attention lets each token measure its \textbf{relevance to other tokens} and build a \textbf{context-aware representation}.
    \item models \textbf{token-to-token relevance}
    \item strengthens \textbf{important context}
    \item creates each output token as a \textbf{weighted combination of previous tokens}
\end{itemize}

\vspace{0.4em}

{\footnotesize Self-Attention}

In \textbf{self-attention}, each token can “look at” \textbf{all other tokens} (including itself). This allows the model to capture:
\begin{itemize}
    \item \textbf{local dependencies} (nearby words)
    \item \textbf{long-range dependencies} (far relationships)
    \item richer \textbf{contextual embeddings}
\end{itemize}

Self-attention produces \textbf{new embeddings} that are more informative than the input embeddings.


\vspace{0.4em}
{\footnotesize Query / Key / Value (Q / K / V)}

Each token is projected (via learned weight matrices) into three vectors:
\begin{itemize}
    \item \textbf{Query (Q):} what the current token is looking for
    \item \textbf{Key (K):} what each token offers as “matchable information”
    \item \textbf{Value (V):} the information each token contributes if selected
\end{itemize}

{\scriptsize How it works:}
\begin{enumerate}[leftmargin=1.8em]
    \item Compare current token’s query with all keys:
    \(Q \cdot K^\top\)
    \item Scale scores by $\sqrt{d_k}$ to keep values stable
    \item Apply a \textbf{mask} (for causal models) to block future tokens
    \item Apply \textbf{softmax} to get attention weights
    \item Compute weighted sum of values:
    \( \text{weights} \times V \rightarrow \text{context output} \)
\end{enumerate}

\textbf{Key insight:}  
Each token becomes a weighted mixture of \textit{all} value vectors, where weights depend on how well the query matches the keys.

\vspace{0.4em}
{\footnotesize Multi-Head Attention}

Transformers run \textbf{multiple attention heads in parallel}. Each head has its own Q/K/V projections, learning a different “view” of the sequence.

Heads often specialize in:
\begin{itemize}
    \item syntax (grammar structure)
    \item semantics (meaning)
    \item coreference (pronouns $\rightarrow$ nouns)
    \item long-range vs local patterns
\end{itemize}

Outputs from heads are:
\begin{itemize}
    \item concatenated
    \item passed through a linear layer to mix into one combined representation
\end{itemize}

\textbf{Analogy:}  
Each head is a different perspective on the sentence — combining them gives deeper understanding.
As embeddings pass through layers of attention, they become increasingly meaningful until the model selects the next word.


\vspace{0.4em}
\hrule
\vspace{0.7em}

\end{landscape}
\end{document}