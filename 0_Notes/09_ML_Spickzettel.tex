\documentclass[a4paper, 10pt]{article}

% import packages
\usepackage[margin=0.2cm]{geometry}
\usepackage{pdflscape}
\usepackage{array}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{titlesec}
\usepackage{float}
\usepackage{needspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{graphicx}


\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}

% global list spacing (affects all levels)
\setlist{noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt}

% now explicitly override indentation for each level you care about:
\setlist[itemize,1]{leftmargin=1.8em}
\setlist[itemize,2]{leftmargin=1.4em}
\setlist[itemize,3]{leftmargin=1em}

\setlength{\columnseprule}{0.1pt}
% ------------------------------------------------------


% Configure makecell package
\renewcommand{\theadalign}{bc}
\renewcommand{\theadgape}{\Gape[4pt]}
\renewcommand{\cellgape}{\Gape[4pt]}
\renewcommand{\cellalign}{lt}

% Only change level 1 itemize bullets
\setlist[itemize,1]{label=\raisebox{0.5ex}{\scalebox{0.6}{$\bullet$}}}
% global line spacing
\renewcommand{\baselinestretch}{1.2}

% colour definitions
\definecolor{lightergray}{gray}{0.90}

\providecommand{\tightlist}{%
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}}

\begin{document}
  \begin{landscape}

  
%\begin{multicols}{4}

{\tiny

{\footnotesize\textbf{Linear Regression}} Supervised Regression
  \begin{itemize}
    \item \textbf{Input / Output:} Kontinuierlich
    \item \textbf{Annahme:} 1 Lösung weil Ableitung 0
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Zielvariabel festlegen
        \item features selection
        \item Encoding Kategorisch
        \item Standardising Nummerisch
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item Summe von Input variabeln und Gewichten \(\beta\)
        \item Fixe ausprogrammierte Formel
        \item Modell lernt die \(\beta\)
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item Residuen \(y- \hat{y} = \epsilon\) (zusammenfassing von \(\epsilon\) ist Metrik)
        \item Mean Absolute Error or Mean Sqare Error
        \item Gibt auch custom metriken, ist problemabhängig (wie mit ausreisser umgehen?, etc)
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item Guides Optimisation
        \item In Linear Regression kann auch MSE sein
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item Gradient Descent
        \item Analytische Methode
    \end{itemize}
    \item \textbf{Pro} niedrige gefahr für overfitting
    \item \textbf{Con} kann sich nicht gut an daten anpassen (starr), kann keine Krümmungen lernen
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Logistic Regression}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item Kategorische vorgegebene Zielvariabel
        \item Features werden gewählt
        \item Kategorische Features Encoden
        \item Wenn Regularisiert (L1/L2) \(\rightarrow\) Standardisiert
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item Modifikation Lineares Modell
        \item Output zwischen 0-1 (wahrscheinlichkeit für eine Klasse)
        \item Eg. Sigmpoid / Logistik  \(\rightarrow\) resultat von Modell in diese Funktion (z) einsetzten
        \item \(\phi(z) = \dfrac{1}{1 + e^-z}\) (definition von funktion Z)
        \item \(p(y=1|x_i) = \phi(\beta_0 + x_i\beta_1)\) (Wahrscheinlichkeit klasse y)
        \item \(p(y=0|x_i) = 1- \phi(\beta_0 + x_i\beta_1)\) (Wahrscheinlichkeit andere klasse als y)
        \item \(\hat{y} = arg \; max  \;  p(y = k | x_1)\) (maximum davon \(\rightarrow\) predicted klasse)
        \item Kann auch konfiguriert werden dass es schon ab eg. 20\% Klasse 1 predicted
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item Was sind für Fälle möglich pro Datenpunkt
        \item Accuracy (nicht gut für inbalanced)
        \item F1-Score (gut bei inbalanced)
        \item Confusion Matrix keine Metrik und nicht immer aussagekräftig
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{K-Nearest Neighbours}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Support Vector Machine}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Decision Trees}} Supervised Classification
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Principal Component Analysis}} Unsupervised Dimensionality Reduction
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Non-Negative Matrix Factorisation}} Unsupervised Dimensionality Reduction
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Auto Encoder}} Unsupervised Dimensionality Reduction
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\footnotesize\textbf{Fully Connected Neural Network}} Typ: ???
  \begin{itemize}
    \item \textbf{Beschreibung}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Data Specification}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Model}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Metrik}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Kostenfunktion}
    \begin{itemize}
        \item 
    \end{itemize}
    \item \textbf{Optimierung}
    \begin{itemize}
        \item 
    \end{itemize}
  \end{itemize}
}
%\end{multicols}

\newpage

%\begin{multicols}{3}

{\tiny

{\normalsize{\textbf{Optimisation}}}

{\footnotesize{\textbf{Metrik}}}
\begin{itemize}
  \item \textbf{Regression}
  \begin{itemize}
      \item \textbf{Mean Absolute Error (MAE)} \\
      \(\frac{1}{n}\sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|\) \\
      robust gegen ausreisser \\
      Fehler $>$ 1 start bestraft \\
      Fehler $<$ 1 schwächer bestraft
  \item \textbf{Mean Square Error (MSE)} \\
  \(\frac{1}{n}\sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2\) \\
  beieinflusst von Ausreisser \\
  bestraft grosse Fehler stark
  \end{itemize}
  \item \textbf{Klassifikation}
  \begin{itemize}
        \item True Positive \(\rightarrow\) TP / True Negative \(\rightarrow\) TN, etc
      \item \textbf{Accuracy} \(\dfrac{TP + TN}{TP + TN + FP + FN}\) \\ 
      irreführend bei inbalanced klassen (99 A und 1 B, wenn immer A dann gute accuracy auch wenn Modell dumm)
      \item \textbf{Precision} \\ \(\dfrac{TP}{TP + FN}\)
      \vspace{0.4em}
      \item \textbf{Recall} \\ \(\dfrac{TP}{TP + FN}\)
      \vspace{0.4em}
      \item \textbf{F1-Score} \\ \(F_1 = 2 \cdot \dfrac{Precision \cdot Recall}{Precision + Recall}\) \\ gut bei inbalanced klassen
  \end{itemize}
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Kostenfunktion}}}
\begin{itemize}
    \item \textbf{Maximum Likelihood}
    \begin{itemize} 
    \item TODO: furuther investigation
        \item we wish to maximize the conditional probability of observing the data (X) given a specific probability distribution and its parameters
        \item \( p(\overrightarrow{y} | X ) = \prod \phi(x^i\beta) \cdot \prod{1-\phi(x^i\beta)}\) \\
        probability feature \(x_i \rightarrow\) +ve sample * -ve Sample 
        \item Wert möglichst nahe 1 \(\rightarrow\) Wahrscheinlichkeit für eine Klasse
        \item Resultat: Wie wahrschienlich sind die labels die ich habe und unserem Modell
        \item Je mehr Datenpunkte desto näher bei 0, aber immernoch vergleichbar
    \end{itemize}
\end{itemize}


\vspace{0.4em}
{\footnotesize{\textbf{Optimisation Algorithm}}}
      \begin{itemize}
      \item {\textbf{Analytische Methode}}
      \begin{itemize}
          \item nicht iterativ (löst die Normalgleichung)
          \item besser bei wenigen features
          \item Matrix muss invertierbar sein
      \end{itemize}
      \item \textbf{Gradient Descent}
      \begin{itemize}
          \item Iterativ, Start zufällige \(\beta\)
          \item Ableitung der Kostenfunktion
          \item mit jedem Schritt in die bessere richtung, bis zum minimum
          \item gut für viele Daten und \(\beta\)
      \end{itemize}
    \end{itemize}


\vspace{0.4em}
\hrule
\vspace{0.7em}

{\normalsize{\textbf{Feature Preprocessing}}}

{\footnotesize{\textbf{Encoding}}}

\begin{itemize}
    \item Information in anderer Maschienen freundlichen Art darzustellen.
    \item oft notwendig für text
\item mit oder ohne Informationsverlust

\item \textbf{Kategorisch}
\begin{itemize}
    \item \textbf{Ordinal}
    \begin{itemize}
        \item mapping von kategorie mit aufsteigender Zahl 0-n
        \item 1 Feature \(\rightarrow\) 1 Feature
        \item hat eine Ordnung, grösse der Zahl hat einen Einfluss!
        \item nicht gut wenn es keine Ordnung hat oder Abstände nicht regelmässig
    \end{itemize}
    \item \textbf{One-Hot}
    \begin{itemize}
        \item n-Einzigartige Eigenschaften \(\rightarrow\) n-Dimensionalen Vektoren
        \item 1 Feature \(\rightarrow\) n Feature
    \end{itemize}
\end{itemize}

\item Meistens One-Hot weil Dinge keine Ordnung haben
\item Möglich mit Domänen-Wissen eigene Encoding machen (eg. Kategorien zusammenfassen)
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Feature Selection}}}

\begin{itemize}
    \item Auswahr von Features die relevant sind, einfacher für das Modell
    \item Ansätze
    \begin{itemize}
        \item Manuell mit Domänenwissen (schwierig)
        \item Automatisch Ausschliessen von features die nichts mit der Zielvariabel zu tun haben
    \end{itemize}
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Standardise}}}
\begin{itemize}
    \item \texttt{StandardScaler} = \(\dfrac{Wert - Durchschnitt}{Standardabweichung}\)
    \item Schiebt alles um 0 Punkt
    \item Entfernt Einheiten, separat pro Feature, erhält Verhältnisse
    \item notwendiger Schritt sodass Modelle richtig funktionieren
    \item Wann?: Parameter müssen vergleichbar sein
    \item Wann?: Distanzen im input space spielen eine Rolle, ansonsten haben grosse Werte einen einfluss
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Dimensionality Reduction}}}

\vspace{0.4em}
\hrule
\vspace{0.7em}

{\normalsize{\textbf{Feature Engineering}}}

{\footnotesize{\textbf{Explizit}}}

\begin{itemize}
    \item Bestehende features \(\rightarrow\) neue features
\item expertenwissen, nimmt einen Teil des Learning Ab \(\rightarrow\) gibt wissen vor.

\item {\textbf{Polynomielle Regression}} (Standard)
\begin{itemize}
    \item Input Space: \texttt{x}
    \item Feature Engineering \(\rightarrow\) Feature Space: \texttt{x}, \texttt{$x^2$}
    \item Modell \(\rightarrow\) Output Space: \texttt{y}
\end{itemize}

\item E.g. Volumen von Fisch berechnen, so kann regression Polynome haben
\item Sind besser auf trainings daten, nicht immer besser auf test daten
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Kernel Trick}}}

\vspace{0.4em}
\hrule
\vspace{0.7em}


{\normalsize{\textbf{Modell Komplexität}}}

{\footnotesize{\textbf{Overfitting vs. Underfitting}}}

\begin{itemize}
    \item \textbf{Overfitting}: super training, schelcht test
    \item \textbf{Underfitting}: schlecht training schlecht test
    \item Je komplexer das Modell desto höher die Gefahr für overfitting
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Cross Validation}}}

\begin{itemize}
    \item vergleich von verschiedenen ML Modell-Arten
    \item Gefahr: falsches Modell, nur durch Zufall gut, mehr validierungsdaten, desto unwahrscheinlicher
    \item 2 Strategien
    \begin{itemize}
        \item Train, Val, Test (no peeking)
        \item k-Fold mehr validierungsdaten herbeizaubern
    \end{itemize}
    \item Alle Daten sind im validierungsset
    \item Iterativ üver alle Segmente gehen
    \item Mehr rechenzeit, aber weil wenig daten nicht ein problem
\end{itemize}

\vspace{0.4em}
{\footnotesize{\textbf{Regularisation}}}
\begin{itemize}
    \item Annahme: Grosse \(\beta\) zeichen für overfitting, sollten nahe 0 sein, Modell wird einfacher
    \item Daten müssen Standard Skaliert werden
    \item Regularisierungsstärke \(\lambda \rightarrow\) ist Hyperparameter
    \item Grosses \(\lambda \rightarrow\) stark konfiguriert \(\beta\) näher bei 0
    \item {\textbf{L1 Lasso}}: Absolut \(|\beta|\)
    \begin{itemize}
        \item Grosse \(|\beta|\) werden bestraft
        \item Gut für automatische Feature-Selection (unwichtiges \(|\beta|\) verschwindet)
        \item \(J_{\text{Reg}}(\beta) = J(\beta) + \lambda \sum_{j=1}^{p} |\beta_j|\)
    \end{itemize}
        \item {\textbf{L2 Ridge}}: Quardrat \(\beta^2\)
    \begin{itemize}
        \item \(|\beta|\) nahe 0 sonst bestraft
        \item Zahlen unter 1 weniger bestraft als bei L1
        \item Alle \(|\beta|\) klein, ausgewogen in vergleich zu L1
        \item \(J_{\text{Reg}}(\beta) = J(\beta) + \lambda \sum_{j=1}^{p} \beta^2_j\)
    \end{itemize}
\end{itemize}

\vspace{0.4em}
\hrule
\vspace{0.7em}


{\normalsize{\textbf{Model Selection}}}

{\footnotesize{\textbf{Hyper Parameter}}}
\begin{itemize}
    \item Werden nicht gelernt \(\rightarrow\) konfiguriert das Modell
    \item Z.B. Regularisierungsstärke
    \item Manuell: Erfahrung \& Theorie  \(\rightarrow\) schwierig
    \item Suchen: Ausprobieren und auf ungesehen Daten merken  \(\rightarrow\) Rechenintensiv
    \begin{itemize}
        \item \textbf{Grid Search:} Liste von Werten, ausprobieren, nicht gut bei vielen hyper Parameter wegen kombination
        \item \textbf{Randomised Search:} Zufällige Zahlen innerhalb eines vorgegebenen Bereiches
    \end{itemize}
\end{itemize}


\vspace{0.4em}
{\footnotesize{\textbf{Algorithm Selection}}}

\vspace{0.4em}
\hrule
\vspace{0.7em}
} % end tiny

%\end{multicols}


\newpage

%\begin{multicols}{3}

\tiny{

{\normalsize{Wie Problem Angehen?}}
\begin{itemize}
    \item \textbf{Problem Verstehen:} Klassifikation, Regression, etc.?
    \item \textbf{Ziel Verstehen:} Einfluss auf Metrik Auswahl
    \item \textbf{Verstehe / Evaluiere Daten:} genug? Garbage-in Grabage-out
    \item \textbf{Datenlandschaft Evaluieren:} crawlable, API, etc.?
    \item \textbf{Domänen Experten:} Was ist wichtig (Feature Selection). Bekannte Zusammenhänge (feature engineering)
    \item \textbf{Datenqualität:}
    \begin{itemize}
        \item verstehen
        \item Standardisieren, Encoden
        \item Inkonsitenz finden \(\rightarrow\) erklären oder ausschliessen
        \item Visualisieren \& Analysieren
        \item Muster mit Domänen Experten bestätigen
        \item Zusammenhänge überprüfen
    \end{itemize}
\end{itemize}

\vspace{0.4em}

%\end{multicols}

} % end tiny


\end{landscape}
\end{document}